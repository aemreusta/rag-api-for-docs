# LLM Router

## Embedding Defaults

While the LLM Router selects chat models, the embedding stack is configured separately and now defaults to Google Gemini `gemini-embedding-001` with `EMBEDDING_DIM=3072`. Change via environment and ensure DB schema alignment.

## LLM Router Architecture

This document describes the LLM Router implementation, which provides a robust mechanism for routing LLM requests across multiple providers with automatic fallback capabilities.

## Overview

The LLM Router is designed to provide high availability for LLM services by implementing:

1. **Priority-based routing** across multiple LLM providers
2. **Automatic fallback** on errors (timeouts, rate limits, authentication failures)
3. **Redis-based caching** of failed providers with cooldown periods
4. Support for both **streaming** and **non-streaming** responses
5. Configurable timeouts for each provider
6. **Structured logging** with correlation IDs and error tracking

## Architecture

The router follows this provider priority sequence:

1. **OpenRouter** (Gemini models)
2. **Groq** (Llama 3 models)
3. **OpenAI** (GPT models)
4. **Local** (for development/fallback)

![LLM Router Flow](diagrams/llm_router_flow.mmd)

For detailed Groq rate limiting flow, see: [Groq Rate Limiting Flow](diagrams/groq_rate_limiting_flow.mmd)

## Implementation

The implementation is based on the following components:

### Base Provider Interface

All LLM providers implement a common interface:

```python
class LLMProvider(ABC):
    @abstractmethod
    def is_available(self) -> bool:
        """Check if the provider is available."""
        pass

    @abstractmethod
    async def complete(self, messages: list[ChatMessage], **kwargs) -> CompletionResponse:
        """Generate completion for the given messages."""
        pass

    @abstractmethod
    async def stream_complete(
        self, messages: list[ChatMessage], **kwargs
    ) -> AsyncGenerator[CompletionResponse, None]:
        """Generate streaming completion for the given messages."""
        pass
```

### Provider Implementations

Each provider implements the interface with specific logic for:

- Availability detection via API keys
- Completion generation
- Streaming response handling
- Error handling and timeout management
- **Structured logging** for monitoring and debugging

#### Groq Provider Features

The Groq provider includes advanced rate limiting and resilience features:

**Rate Limit Management**:

- **Header parsing**: Extracts `x-ratelimit-*` headers (requests, tokens, reset times)
- **Cross-worker coordination**: Shares quota information via Redis across Gunicorn workers
- **Preemptive skip**: Avoids API calls when quota is critically low (≤2 requests or ≤1000 tokens)
- **Intelligent retry**: Honors `retry-after` headers with exponential backoff and jitter

**Redis Integration**:

- Stores rate limit info under `groq:rate_limit` key with 5-minute TTL
- Enables consistent quota tracking across multiple worker processes
- Prevents wasted API calls through shared state management

**Prometheus Metrics**:

- `llm_provider_rate_limit_hits_total` counter with labels:
  - `provider="groq"` and `action="hit"` for rate limit violations
  - `provider="groq"` and `action="preemptive_skip"` for avoided calls

**Error Handling**:

- Treats 498 (capacity exceeded) same as 429 (rate limited)
- Implements jitter (±25%) to prevent thundering herd effects
- Respects retry-after headers when provided by Groq API
- Falls back to exponential backoff when retry-after is not available

### Router Implementation

The `LLMRouter` class:

1. Initializes all available providers
2. Manages Redis connection for caching failed providers
3. Implements fallback logic when providers fail
4. Provides both synchronous and asynchronous interfaces
5. **Maintains structured logging** with proper correlation IDs

#### Recent Improvements (v2025.01.10)

**Groq Provider Hardening**:

- **Comprehensive rate limiting**: Full implementation of Groq API rate limit handling
- **Header parsing**: Extracts all `x-ratelimit-*` headers with proper type conversion
- **Cross-worker coordination**: Redis-based quota sharing for consistent limits
- **Intelligent retry logic**: Exponential backoff with jitter and retry-after support
- **Preemptive rate limiting**: Skips API calls when quota is critically low
- **Enhanced metrics**: Added Prometheus counters for rate limit events
- **Code refactoring**: Reduced complexity from 17 to <10 for maintainability

**Logger Implementation Fix** (v2024.12.30):

- Fixed `AttributeError: 'LLMRouter' object has no attribute 'logger'`
- Implemented proper Pydantic-compatible logger using `PrivateAttr()`
- Added `@property` decorator for clean public interface
- Integrated with existing structured logging framework

**Metadata Standardization**:

- Verified Groq Llama 3 models expose an **8192-token context window** and updated metadata accordingly.
- Ensured consistent metadata across all router instances

**Error Handling Improvements**:

- Enhanced fallback decision logic for malformed requests
- Improved exception chaining in Groq provider retry logic
- Added comprehensive error classification and logging

## Configuration

The router is configured via environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENROUTER_API_KEY` | API key for OpenRouter | None |
| `GROQ_API_KEY` | API key for Groq | None |
| `GROQ_MODEL_NAME` | Groq model to use | `"llama3-70b-8192"` |
| `GROQ_TIMEOUT_SECONDS` | Groq-specific timeout | `30` |
| `OPENAI_API_KEY` | API key for OpenAI | None |
| `LLM_MODEL_NAME` | Default model to use | `"openai/gpt-3.5-turbo"` |
| `LLM_TIMEOUT_SECONDS` | Timeout for LLM requests | `30` |
| `LLM_FALLBACK_CACHE_SECONDS` | Cooldown period for failed providers | `300` (5 minutes) |
| `REDIS_URL` | Redis connection URL | `"redis://redis:6379/0"` |

## Usage

### Basic Usage

```python
from app.core.llm_router import LLMRouter
from llama_index.core.llms import ChatMessage

router = LLMRouter()

# Non-streaming completion
response = router.complete(
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="Tell me about LLMs.")
    ]
)
print(response.text)  # Use .text for the actual response content

# Streaming completion
async for chunk in router.astream_complete(
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="Tell me about LLMs.")
    ]
):
    print(chunk.text, end="", flush=True)
```

### Integration with Query Engine

The LLM Router integrates with the existing query engine:

```python
from app.core.llm_router import LLMRouter
from app.core.query_engine import get_query_engine

# Initialize the LLM router
router = LLMRouter()

# Pass the router to the query engine
query_engine = get_query_engine(llm=router)

# Use the query engine with automatic fallback capability
response = query_engine.query("What is in the document?")
```

## Error Handling

The router implements sophisticated error handling:

1. **Provider-specific errors** are caught and logged with correlation IDs
2. **Timeouts** are handled gracefully with fallback to the next provider
3. **Rate limits** trigger fallback and temporary caching of failed providers
4. **Authentication failures** are detected and handled appropriately
5. **Malformed requests** are classified as client errors (no fallback)
6. **Exception chaining** preserves original error context

### Error Classification

The router classifies errors into categories:

- `TIMEOUT`: Request timeouts and network delays
- `RATE_LIMIT`: API rate limiting (429 errors)
- `AUTH_FAILURE`: Authentication/authorization failures (401/403)
- `API_ERROR`: Service errors and token limits (503/413)
- `NETWORK_ERROR`: Connection and network issues

## Redis Caching

Failed providers are cached in Redis with:

- Provider name and error type as composite key
- Error metadata and timestamp as value
- Configurable cooldown period (default: 5 minutes)
- Automatic cleanup after cooldown expires

This prevents repeated failures from the same provider during the cooldown period.

## Logging and Monitoring

The router provides comprehensive logging:

- **Structured JSON logs** with correlation IDs
- **Provider selection** and fallback decisions
- **Error tracking** with full context
- **Performance metrics** for timeout and success rates
- **Redis operations** for caching behavior

Example log entries:

```json
{
  "event": "Trying provider",
  "provider": "groq",
  "model": "llama3-70b-8192",
  "request_id": "abc123",
  "timestamp": "2024-12-30T12:00:00Z"
}
```

## Testing

The router includes comprehensive tests:

- **37 total tests** with 100% pass rate (LLM router module)
- Unit tests for each provider implementation
- Mocked Redis for testing caching behavior
- Timeout and error simulation tests
- **Groq-specific tests**: Rate limiting, retry logic, header parsing, preemptive skipping
- **Smoke tests**: Optional live API tests when environment variables are available
- End-to-end tests with fallback scenarios

### Test Coverage

- **Provider implementations**: OpenRouter, Groq, OpenAI, Local
- **Router logic**: Initialization, fallback, caching, streaming
- **Error handling**: Classification, fallback decisions, Redis caching
- **Integration**: Query engine integration, async/sync interfaces

## Future Improvements

Potential future enhancements:

1. **Cost-based routing** to optimize for price/performance
2. **Load balancing** across providers of the same priority
3. **Adaptive timeouts** based on historical performance
4. **Circuit breaker pattern** for more sophisticated failure handling
5. **Additional providers** (Anthropic, Cohere, etc.)
6. **Performance analytics** dashboard for provider monitoring
