# LLM Router Architecture

This document describes the LLM Router implementation, which provides a robust mechanism for routing LLM requests across multiple providers with automatic fallback capabilities.

## Overview

The LLM Router is designed to provide high availability for LLM services by implementing:

1. **Priority-based routing** across multiple LLM providers
2. **Automatic fallback** on errors (timeouts, rate limits, authentication failures)
3. **Redis-based caching** of failed providers with cooldown periods
4. Support for both **streaming** and **non-streaming** responses
5. Configurable timeouts for each provider

## Architecture

The router follows this provider priority sequence:

1. **OpenRouter** (Gemini models)
2. **Groq** (Llama 3 models)
3. **OpenAI** (GPT models)
4. **Local** (for development/fallback)

![LLM Router Flow](diagrams/llm_router_flow.mmd)

## Implementation

The implementation is based on the following components:

### Base Provider Interface

All LLM providers implement a common interface:

```python
class LLMProvider(ABC):
    @abstractmethod
    def is_available(self) -> bool:
        """Check if the provider is available."""
        pass

    @abstractmethod
    async def complete(self, messages: list[ChatMessage], **kwargs) -> CompletionResponse:
        """Generate completion for the given messages."""
        pass

    @abstractmethod
    async def stream_complete(
        self, messages: list[ChatMessage], **kwargs
    ) -> AsyncGenerator[CompletionResponse, None]:
        """Generate streaming completion for the given messages."""
        pass
```

### Provider Implementations

Each provider implements the interface with specific logic for:

- Availability detection via API keys
- Completion generation
- Streaming response handling
- Error handling and timeout management

### Router Implementation

The `LLMRouter` class:

1. Initializes all available providers
2. Manages Redis connection for caching failed providers
3. Implements fallback logic when providers fail
4. Provides both synchronous and asynchronous interfaces

## Configuration

The router is configured via environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENROUTER_API_KEY` | API key for OpenRouter | None |
| `GROQ_API_KEY` | API key for Groq | None |
| `OPENAI_API_KEY` | API key for OpenAI | None |
| `LLM_MODEL_NAME` | Default model to use | `"openai/gpt-3.5-turbo"` |
| `LLM_TIMEOUT_SECONDS` | Timeout for LLM requests | `30` |
| `REDIS_URL` | Redis connection URL | `"redis://redis:6379/0"` |
| `REDIS_PREFIX` | Redis key prefix | `"llm_router:"` |
| `PROVIDER_COOLDOWN_SECONDS` | Cooldown period for failed providers | `300` (5 minutes) |

## Usage

### Basic Usage

```python
from app.core.llm_router import LLMRouter
from llama_index.core.llms import ChatMessage

router = LLMRouter()

# Non-streaming completion
response = router.complete(
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="Tell me about LLMs.")
    ]
)
print(response.message.content)

# Streaming completion
async for chunk in router.stream_complete(
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="Tell me about LLMs.")
    ]
):
    print(chunk.message.content, end="", flush=True)
```

### Integration with Query Engine

The LLM Router integrates with the existing query engine:

```python
from app.core.llm_router import LLMRouter
from app.core.query_engine import get_query_engine

# Initialize the LLM router
router = LLMRouter()

# Pass the router to the query engine
query_engine = get_query_engine(llm=router)

# Use the query engine with automatic fallback capability
response = query_engine.query("What is in the document?")
```

## Error Handling

The router implements sophisticated error handling:

1. **Provider-specific errors** are caught and logged
2. **Timeouts** are handled gracefully with fallback to the next provider
3. **Rate limits** trigger fallback and temporary caching of failed providers
4. **Authentication failures** are detected and handled appropriately

## Redis Caching

Failed providers are cached in Redis with:

- Provider name as key
- Error type and timestamp as value
- Configurable cooldown period (default: 5 minutes)

This prevents repeated failures from the same provider during the cooldown period.

## Testing

The router includes comprehensive tests:

- Unit tests for each provider
- Mocked Redis for testing caching behavior
- Timeout and error simulation tests
- End-to-end tests with real API calls (when credentials are available)

## Future Improvements

Potential future enhancements:

1. **Cost-based routing** to optimize for price/performance
2. **Load balancing** across providers
3. **Adaptive timeouts** based on historical performance
4. **Circuit breaker pattern** for more sophisticated failure handling
5. **Additional providers** (Anthropic, Cohere, etc.)
