# Development Environment Setup

This document outlines our Docker-based development environment setup, which ensures consistency across all development machines and matches our production environment.

## Prerequisites

- Docker Desktop (latest version)
- Git
- A text editor (VS Code recommended)
- Make (for using Makefile shortcuts)

## Quick Setup

```bash
# 1. Clone the repository
git clone https://github.com/your-org/chatbot-api-service.git
cd chatbot-api-service

# 2. Configure environment
cp .env.example .env
# Edit .env with your API keys

# 3. Start everything
make up

# 4. Run initial data ingestion
make ingest

# 5. Verify system
curl http://localhost:8000/health
```

### Development Environment Access

- API: <http://localhost:8000>
- API Docs: <http://localhost:8000/docs>
- Langfuse: <http://localhost:3000>

## Available Make Commands (17 Total)

### System Management

```bash
make up                     # Start all services (recommended)
make down                  # Stop all services  
make logs                  # View logs from all containers
make help                  # Show all available commands
make health-check          # Comprehensive system health verification
```

### Development & Testing

```bash
make shell                 # Open bash shell in app container
make test                  # Run all tests (95 tests should pass)
make test-cov             # Run tests with coverage report
make test-pgvector        # pgvector performance & configuration tests
make test-metrics         # flexible metrics system tests
make benchmark            # Vector search performance benchmarks
make lint                 # Check code quality with ruff
make format               # Format code with ruff
make quality-check        # All quality checks (lint + format + type)
make chat                 # Test local chat endpoint with curl
```

### Database Operations

```bash
make db-shell             # Open PostgreSQL shell
make db-status            # Database & pgvector status information
make ingest              # Run data ingestion (index PDF documents)
```

### Maintenance & Dependencies

```bash
make clean                # Remove all containers and volumes
make clean-pyc            # Remove Python cache files
make deps-install         # Install/sync all dependencies with pip-tools
make deps-update          # Update all dependencies to latest versions
make deps-compile         # Compile requirements.txt from requirements.in
```

## Environment Configuration

### Required Environment Variables

Edit `.env` with your actual API keys:

```env
# Database (automatically configured for Docker)
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/app
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=app

# OpenRouter API (REQUIRED - Get from openrouter.ai)
OPENROUTER_API_KEY=your_openrouter_api_key_here
LLM_MODEL_NAME=google/gemini-1.5-pro-latest

# Langfuse Observability (REQUIRED - Get from langfuse.com)
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key_here
LANGFUSE_SECRET_KEY=your_langfuse_secret_key_here
LANGFUSE_HOST=http://langfuse:3000

# Application Security (REQUIRED - Generate strong random keys)
API_KEY=your_very_strong_api_key_here
ADMIN_API_KEY=your_admin_api_key_here

# Langfuse Service Configuration
NEXTAUTH_SECRET=your_strong_nextauth_secret_here
SALT=your_strong_salt_here

# Flexible Monitoring Configuration (NEW)
METRICS_BACKEND=auto  # auto/prometheus/datadog/opentelemetry/noop
PROMETHEUS_ENABLED=true
# DATADOG_API_KEY=your_datadog_api_key  # If using DataDog
EMBEDDING_DIM=1536  # Vector embedding dimension
```

### Optional Settings

```env
DEBUG=true
ENVIRONMENT=development
LOG_LEVEL=INFO
PDF_DOCUMENTS_DIR=pdf_documents
```

## Docker Services

### Current Stack

- FastAPI Application: Main API server with live reload
- PostgreSQL + pgvector: Database with vector similarity search
- Redis: In-memory cache for session management
- Langfuse: LLM observability and tracing

### Service Health Checks

All services include health checks and dependency management:

- PostgreSQL: `pg_isready` check
- Redis: `redis-cli ping` check
- App: Depends on healthy database and cache

### Volume Mappings

- `./app:/app/app`: Live code reloading
- `./pdf_documents:/app/pdf_documents`: Document storage
- `./tests:/app/tests`: Test directory
- Database and Redis data persisted in named volumes

## Development Workflow

### 1. Daily Development

```bash
# Start your dev session
make up

# View logs if needed
make logs

# Run tests after changes
make test

# Format and lint code
make format
make lint
```

### 2. Adding New Features

```bash
# Create feature branch
git checkout -b feat/your-feature-name

# Make changes, then test
make test
make lint

# Commit with conventional commits
git commit -m "feat(scope): add new feature"
```

### 3. Working with Dependencies

We use `pip-tools` to manage Python dependencies. This ensures reproducible builds and helps maintain clean dependency trees.

```bash
# Add new dependency to requirements.in
echo "new-package" >> requirements.in

# For development dependencies
echo "new-dev-package" >> requirements-dev.in

# Compile requirements.txt from requirements.in
pip-compile requirements.in

# Compile development requirements
pip-compile requirements-dev.in

# Upgrade all packages to their latest versions
pip-compile --upgrade requirements.in
pip-compile --upgrade requirements-dev.in

# Install exact versions from compiled requirements
pip-sync requirements.txt requirements-dev.txt

# Rebuild container with new dependencies
make down
make up
```

### Important Dependency Management Notes

1. Production vs Development:
   - Add core dependencies to `requirements.in`
   - Add development tools to `requirements-dev.in`

2. Version Pinning:
   - For critical dependencies: `package==1.2.3`
   - For flexible dependencies: `package>=1.2.3`

3. After Adding Dependencies:
   - Run `pip-compile` to update the `.txt` files
   - Commit both `.in` and `.txt` files
   - Rebuild containers with `make down && make up`

4. Dependency Conflicts:
   - If you encounter conflicts, check the error messages from pip-compile
   - Adjust version constraints in the `.in` files
   - Use `pip-compile --upgrade-package package==version` for targeted upgrades

### 4. Database Operations

```bash
# Access database shell
make db-shell

# Re-run data ingestion
make ingest

# Check database tables
make db-shell
\dt  # List tables
\d charity_policies  # Describe vector table
```

### 5. Debugging

```bash
# View all service logs
make logs

# View specific service logs
docker-compose logs app
docker-compose logs postgres

# Access app container for debugging
make shell
python  # Interactive Python
```

## Testing & Quality Assurance

### Running Tests

```bash
# Run all tests (should show 95/95 passing)
make test

# Run with coverage report
make test-cov

# Test pgvector performance and configuration
make test-pgvector

# Test flexible metrics system
make test-metrics

# Run vector search performance benchmarks
make benchmark

# Expected output: 95 tests passing, 0 warnings
```

### Performance Benchmarks

```bash
# Run comprehensive vector search benchmarks
make benchmark

# Expected results:
# - P99 latency: ~1.88ms (target <50ms)
# - 100% recall accuracy at top-1, top-5, top-10
# - HNSW index performance validation
```

### Code Quality

```bash
# Format code
make format

# Check linting (should pass cleanly)
make lint
```

### Pre-commit Hooks

The repository includes comprehensive pre-commit hooks:

- **Code formatting** with Ruff
- **Security scanning** with Gitleaks
- **Markdown linting**
- **YAML/TOML validation**
- **Conventional commit enforcement**

Install hooks: `pre-commit install`

## Troubleshooting

### Common Issues

1. **Services Won't Start**

   ```bash
   # Check what's using ports
   lsof -i :8000
   lsof -i :5432
   
   # Clean restart
   make down
   docker system prune -f
   make up
   ```

2. **Database Connection Issues**

   ```bash
   # Verify PostgreSQL is healthy
   docker-compose ps
   
   # Check database logs
   docker-compose logs postgres
   
   # Recreate database
   make down
   docker volume rm chatbot-api-service_postgres_data
   make up
   make ingest
   ```

3. **Ingestion Fails**

   ```bash
   # Check if pgvector extension is enabled
   make db-shell
   SELECT * FROM pg_extension WHERE extname = 'vector';
   
   # Re-enable if needed
   CREATE EXTENSION IF NOT EXISTS vector;
   ```

4. **Permission Issues**

   ```bash
   # Fix file ownership
   sudo chown -R $USER:$USER .
   
   # Fix Docker permissions
   docker-compose down
   docker-compose up
   ```

### Health Check Commands

```bash
# Comprehensive system health check
make health-check

# API health
curl http://localhost:8000/health

# Database and pgvector status
make db-status

# Prometheus metrics (if enabled)
curl http://localhost:8000/metrics

# Database connection
make db-shell -c "SELECT 1;"

# Redis connection  
docker-compose exec redis redis-cli ping

# All services status
docker-compose ps
```

## Monitoring & Observability

### Available Monitoring Backends

The system supports flexible monitoring through multiple backends:

```bash
# Development (no metrics overhead)
METRICS_BACKEND=noop

# Prometheus (default)
METRICS_BACKEND=prometheus
PROMETHEUS_ENABLED=true

# DataDog (enterprise)
METRICS_BACKEND=datadog
DATADOG_API_KEY=your-api-key

# Auto-detection (recommended)
METRICS_BACKEND=auto  # Automatically selects best available
```

### Monitoring Endpoints

- **API Metrics**: <http://localhost:8000/metrics> (Prometheus format)
- **API Health**: <http://localhost:8000/health>
- **Langfuse Observability**: <http://localhost:3000>

### Available Metrics

- `vector_search_duration_seconds`: Vector search latency (P99: ~1.88ms)
- `vector_search_requests_total`: Total vector search requests
- `vector_search_recall`: Search recall accuracy (100%)
- `http_requests_total`: HTTP request counts
- `llm_provider_requests_total`: LLM provider usage

## Performance Tips

1. **Use Make Commands**: Always prefer `make up` over `docker-compose up -d`
2. **Volume Optimization**: Use named volumes for better performance
3. **Resource Limits**: Adjust Docker Desktop memory allocation if needed
4. **Cleanup Regularly**: Run `make clean` occasionally to free space

## IDE Integration

### VS Code Setup

Install recommended extensions:

- Python
- Docker
- GitLens
- Ruff (Python linting/formatting)

### Configuration

The repository includes:

- `.vscode/settings.json` for consistent formatting
- `pyproject.toml` for tool configuration
- `.pre-commit-config.yaml` for automated quality checks

## Deployment Preparation

The development environment matches production deployment:

- Same Docker images and configurations
- Environment-based configuration
- Health checks for orchestration
- Comprehensive logging

This ensures smooth deployment and debugging of production issues locally.

## Langfuse Local Setup

### Prerequisites

- Docker and Docker Compose
- Python 3.11 or higher
- Make (optional, but recommended)
- Git

### Quick Start

1. Clone the repository
2. Copy `.env.example` to `.env` and fill in the required values
3. Run `make up` to start all services
4. Visit <http://localhost:8000/docs> for the API documentation

### Services

The application runs several services using Docker Compose:

#### Main Application (FastAPI)

- Port: 8000
- Documentation: <http://localhost:8000/docs>
- Live reload enabled for development

#### PostgreSQL with pgvector

- Port: 5432
- Credentials in `.env`
- Vector storage enabled

#### Redis Cache

- Port: 6379
- No authentication required in development

#### Langfuse Observability

- Port: 3000
- UI: <http://localhost:3000>
- Uses the same PostgreSQL instance (separate database)
- Authentication configured via `.env` secrets
- Provides:
  - LLM request tracing
  - Prompt management
  - Cost tracking
  - Performance monitoring

### Environment Variables

Create a `.env` file in the root directory with the following structure:

```env
# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=app
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/app

# Langfuse Service
NEXTAUTH_SECRET=<strong-random-string>  # For Langfuse UI session security
SALT=<strong-random-string>            # For Langfuse data encryption
LANGFUSE_PUBLIC_KEY=<from-langfuse-ui> # Get after first login
LANGFUSE_SECRET_KEY=<from-langfuse-ui> # Get after first login
LANGFUSE_HOST=http://langfuse:3000

# OpenRouter (LLM Provider)
OPENROUTER_API_KEY=<your-api-key>
LLM_MODEL_NAME=google/gemini-1.5-pro-latest

# Application Security
API_KEY=<strong-random-string>
ADMIN_API_KEY=<strong-random-string>
```

### Development Workflow

1. Start all services:

   ```bash
   make up
   ```

2. Set up Langfuse:
   - Visit <http://localhost:3000>
   - Create an account
   - Get your API keys from the UI
   - Update LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in `.env`

3. Watch logs:

   ```bash
   make logs
   ```

4. Run tests:

   ```bash
   make test
   ```

5. Stop all services:

   ```bash
   make down
   ```

### Troubleshooting

#### Common Issues

1. **PostgreSQL Connection Issues**
   - Check if the container is healthy: `docker ps`
   - Verify DATABASE_URL in `.env`
   - Ensure pgvector extension is enabled

2. **Langfuse UI Not Loading**
   - Verify NEXTAUTH_SECRET and SALT are set in `.env`
   - Check if PostgreSQL is healthy
   - Look for errors in logs: `make logs service=langfuse`

3. **Redis Connection Issues**
   - Check Redis container health
   - Verify REDIS_HOST in environment

#### Getting Help

1. Check the logs: `make logs`
2. Review error messages in the FastAPI docs
3. Consult the project issues on GitHub

## Database Schema

The project utilizes a single PostgreSQL instance that hosts two logically separate databases: `app` for the core chatbot application and `langfuse` for the observability platform.

### Application Database (`app`)

This database contains all the data required for the chatbot's RAG functionality. The schema is managed by Alembic migrations.

#### Table: `content_embeddings`

This is the primary table for our RAG pipeline. It stores the indexed text chunks from the source PDF documents and their corresponding vector embeddings.

| Column Name | Data Type | Description |
|-------------|-----------|-------------|
| `id` | INTEGER (Primary Key) | A unique identifier for each text chunk |
| `source_document` | VARCHAR(255) | The filename of the PDF from which the chunk was extracted (e.g., `charity_policy_v1.pdf`). Essential for citations |
| `page_number` | INTEGER | The page number within the source PDF where the chunk originated. Also used for citations |
| `content_text` | TEXT | The actual text content of the chunk that is shown to the LLM as context |
| `content_vector` | VECTOR(384) | The numerical vector embedding of `content_text`, used for similarity searches |

#### Table: `query_logs`

This table stores user queries and responses for analytics and monitoring purposes.

| Column Name | Data Type | Description |
|-------------|-----------|-------------|
| `id` | INTEGER (Primary Key) | A unique identifier for each query |
| `query_text` | TEXT | The user's input question |
| `response_text` | TEXT | The chatbot's response |
| `session_id` | VARCHAR(255) | Session identifier for tracking conversations |

#### Table: `alembic_version`

This table is managed by Alembic to track database migration versions.

| Column Name | Data Type | Description |
|-------------|-----------|-------------|
| `version_num` | VARCHAR(32) (Primary Key) | Current migration version |

### Langfuse Database (`langfuse`)

This database is managed automatically by the Langfuse container. You should not modify its schema directly. It contains all the data needed for tracing, metrics, evaluations, and user feedback, stored in tables such as `traces`, `observations`, `scores`, `users`, etc.
