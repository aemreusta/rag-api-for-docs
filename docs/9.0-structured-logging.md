# Structured Logging

This document describes the structured logging implementation in the chatbot API service, which provides JSON-formatted logs with correlation IDs, sensitive data masking, and comprehensive observability.

## Overview

The structured logging system uses [structlog](https://structlog.org/) to provide:

- **JSON formatted logs** for easy parsing by log aggregation systems (ELK, Loki, etc.)
- **Correlation IDs** for tracing requests across services and components
- **Sensitive data masking** to prevent secrets from appearing in logs
- **Request/response timing** and business metrics
- **Log rotation** with compression for production deployments

## Architecture

```
Request → Middleware → Business Logic → Langfuse
   ↓           ↓              ↓            ↓
Context Variables  →  Correlation IDs  →  trace_id
   ↓           ↓              ↓            ↓
structlog → JSON Renderer → Log Output
```

### Components

| Component | Purpose | Location |
|-----------|---------|----------|
| **LoggingConfig** | Core setup and configuration | `app/core/logging_config.py` |
| **Middleware** | Request/response logging | `app/core/middleware.py` |
| **SensitiveDataFilter** | Redacts secrets from logs | `app/core/logging_config.py` |
| **Context Variables** | Correlation ID propagation | `app/core/logging_config.py` |

## Configuration

### Environment Variables

```bash
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# SQL query logging level
LOG_LEVEL_SQL=WARNING

# Output format (true=JSON, false=console)
LOG_JSON=true

# Enable file logging
LOG_TO_FILE=false

# Log file path pattern (supports %(process)d, %(asctime)s)
LOG_FILE=logs/app_%(process)d_%Y%m%d.log
```

### Settings in `app/core/config.py`

```python
class Settings(BaseSettings):
    # Structured Logging Configuration
    LOG_LEVEL: str = "INFO"
    LOG_LEVEL_SQL: str = "WARNING"
    LOG_JSON: bool = True
    LOG_TO_FILE: bool = False
    LOG_FILE: str = "logs/app_%(process)d_%Y%m%d.log"
```

## Usage

### Getting a Logger

```python
from app.core.logging_config import get_logger

# Auto-detect module name
logger = get_logger()

# Explicit logger name
logger = get_logger("my.module.name")
```

### Basic Logging

```python
# Structured logging with context
logger.info(
    "User action completed",
    user_id="123",
    action="login",
    duration_ms=250.5
)

# Error logging with exception
try:
    risky_operation()
except Exception as e:
    logger.error(
        "Operation failed",
        operation="risky_operation",
        error=str(e),
        exc_info=True
    )
```

### Correlation IDs

Correlation IDs are automatically managed by middleware, but can be set manually:

```python
from app.core.logging_config import set_request_id, set_trace_id

# Set correlation IDs (auto-generates UUID if not provided)
request_id = set_request_id()
set_trace_id("external-trace-123")

# All subsequent logs will include these IDs
logger.info("Processing request")  # Includes request_id and trace_id
```

## Log Format

### JSON Structure

```json
{
  "event": "Request completed",
  "method": "POST",
  "path": "/api/v1/chat",
  "status_code": 200,
  "duration_ms": 105.44,
  "request_id": "cda689c5-a191-418d-b9ad-8928a587263b",
  "trace_id": "test-trace-12345",
  "client_ip": "192.168.65.1",
  "user_agent": "curl/8.7.1",
  "process_id": 288,
  "logger": "app.core.middleware",
  "level": "info",
  "timestamp": "2025-06-27T19:07:00.150496Z"
}
```

### Console Format (Development)

```
2025-06-27 19:07:00 [info] Request completed [app.core.middleware] method=POST path=/api/v1/chat status_code=200
```

## Sensitive Data Masking

The system automatically redacts sensitive information from logs:

### Patterns Masked

| Pattern | Replacement | Example |
|---------|-------------|---------|
| API Keys | `api_key=[REDACTED]` | `api_key=abc123` → `api_key=[REDACTED]` |
| Bearer Tokens | `Bearer [REDACTED]` | `Bearer xyz789` → `Bearer [REDACTED]` |
| Passwords | `password=[REDACTED]` | `password="secret"` → `password=[REDACTED]` |
| Email Addresses | `[EMAIL_REDACTED]` | `user@example.com` → `[EMAIL_REDACTED]` |
| Secrets | `secret=[REDACTED]` | `secret=mysecret` → `secret=[REDACTED]` |

### Custom Masking

To add new sensitive patterns:

```python
# In app/core/logging_config.py
SENSITIVE_PATTERNS = [
    # Add new pattern
    (re.compile(r'my_secret_field=([A-Za-z0-9]+)', re.IGNORECASE), 'my_secret_field=[REDACTED]'),
    # ... existing patterns
]
```

## Middleware Integration

### Request/Response Logging

The `StructuredLoggingMiddleware` automatically logs:

- **Request start**: Method, path, client IP, user agent
- **Request completion**: Status code, duration, response size
- **Errors**: Exception details with stack traces

### Rate Limiting Events

The `RateLimitLoggingMiddleware` logs rate limiting events:

```json
{
  "event": "Rate limit exceeded",
  "client_ip": "192.168.1.100",
  "path": "/api/v1/chat",
  "method": "POST",
  "retry_after_seconds": "86400",
  "status_code": 429
}
```

## Langfuse Integration

Correlation IDs are shared between logs and Langfuse traces:

```python
# In chat endpoint
trace_id = get_trace_id()

# Create Langfuse trace with same ID
trace = langfuse_client.trace(
    name="chat-request",
    id=trace_id  # Same trace_id as logs
)

logger.info("Processing chat request", trace_id=trace_id)
```

This enables **1-click drill-down** from logs to Langfuse traces.

## Log Rotation

### File Rotation

- **Max Size**: 100MB per file
- **Backup Count**: 5 files
- **Compression**: Automatic gzip compression of rotated files
- **Pattern**: `app_<process_id>_<date>.log`

### Configuration

```python
# Custom rotation handler
from app.core.logging_config import CompressingRotatingFileHandler

handler = CompressingRotatingFileHandler(
    filename="logs/myapp.log",
    maxBytes=100 * 1024 * 1024,  # 100MB
    backupCount=5,
    encoding='utf-8'
)
```

## Production Deployment

### Docker Configuration

Logs are sent to stdout by default for Docker compatibility:

```dockerfile
# No special configuration needed
# Docker's json-file driver captures stdout
```

### ELK Stack Integration

```yaml
# docker-compose.yml
services:
  app:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  
  filebeat:
    image: elastic/filebeat:8.0.0
    # Configure to read Docker logs and send to Elasticsearch
```

### Environment Variables

```bash
# Production settings
LOG_LEVEL=INFO
LOG_JSON=true
LOG_TO_FILE=false  # Use stdout for Docker
LOG_LEVEL_SQL=WARNING
```

## Testing

### Unit Tests

The logging system includes comprehensive tests:

```bash
# Run logging tests
pytest tests/test_structured_logging.py -v

# Categories tested:
# - Sensitive data filtering
# - Correlation ID management
# - Log format validation
# - File rotation
# - Configuration handling
```

### Test Coverage

- **20 tests** covering all major functionality
- **Sensitive data masking** validation
- **Correlation ID** propagation and isolation
- **Configuration** flexibility
- **Error handling** edge cases

## Troubleshooting

### Common Issues

#### Logs Not Appearing

```python
# Check if logging is configured
from app.core.logging_config import setup_logging
setup_logging()

# Verify log level
import logging
logging.getLogger().setLevel(logging.DEBUG)
```

#### Missing Correlation IDs

```python
# Ensure middleware is registered
app.add_middleware(StructuredLoggingMiddleware)

# Check context variables
from app.core.logging_config import get_request_id, get_trace_id
print(f"Request ID: {get_request_id()}")
print(f"Trace ID: {get_trace_id()}")
```

#### Sensitive Data in Logs

```python
# Verify filter is applied
console_handler.addFilter(SensitiveDataFilter())

# Test redaction
from app.core.logging_config import SensitiveDataFilter
filter_instance = SensitiveDataFilter()
# ... test with sample data
```

### Debug Mode

Enable debug logging for troubleshooting:

```bash
LOG_LEVEL=DEBUG
LOG_JSON=false  # Easier to read in development
```

## Best Practices

### Do's

- ✅ Use structured fields instead of string interpolation
- ✅ Include business context (user_id, session_id, etc.)
- ✅ Log errors with `exc_info=True` for stack traces
- ✅ Use appropriate log levels (DEBUG for development, INFO for business events)
- ✅ Include timing information for performance monitoring

### Don'ts

- ❌ Don't log sensitive data without masking
- ❌ Don't use string formatting in log messages
- ❌ Don't log at DEBUG level in production
- ❌ Don't include large payloads in logs
- ❌ Don't forget to set correlation IDs for external calls

### Example: Good Logging

```python
logger.info(
    "Chat request processed",
    session_id=request.session_id,
    question_length=len(request.question),
    answer_length=len(response.answer),
    source_count=len(response.sources),
    processing_time_ms=duration,
    model_used=settings.LLM_MODEL_NAME
)
```

### Example: Bad Logging

```python
# Don't do this
logger.info(f"User {user_id} asked: {question}")  # Sensitive data, string formatting
logger.debug("Processing...")  # No context
logger.error("Error occurred")  # No error details
```

## Monitoring and Alerting

### Key Metrics to Monitor

- **Error rate**: Count of ERROR level logs
- **Response time**: `duration_ms` field distribution
- **Rate limiting**: Count of 429 responses
- **User activity**: Request patterns by IP/session

### Sample Queries

#### Elasticsearch/Kibana

```json
// High error rate alert
{
  "query": {
    "bool": {
      "filter": [
        {"term": {"level": "error"}},
        {"range": {"timestamp": {"gte": "now-5m"}}}
      ]
    }
  }
}

// Slow requests
{
  "query": {
    "range": {
      "duration_ms": {"gte": 1000}
    }
  }
}
```

#### Grafana/Loki

```logql
# Error rate
rate(({job="chatbot-api"} |= "error")[5m])

# 95th percentile response time
quantile_over_time(0.95, {job="chatbot-api"} | json | unwrap duration_ms [5m])
```

## Migration Guide

### From Basic Logging

If migrating from basic Python logging:

1. **Replace logger imports**:

   ```python
   # Old
   import logging
   logger = logging.getLogger(__name__)
   
   # New
   from app.core.logging_config import get_logger
   logger = get_logger(__name__)
   ```

2. **Update log calls**:

   ```python
   # Old
   logger.info(f"User {user_id} completed action")
   
   # New
   logger.info("User completed action", user_id=user_id, action="login")
   ```

3. **Add correlation context**:

   ```python
   # In middleware or request handlers
   from app.core.logging_config import set_request_id
   set_request_id()  # Auto-generates UUID
   ```

### Configuration Migration

Update environment variables:

```bash
# Old
LOGGING_LEVEL=INFO

# New
LOG_LEVEL=INFO
LOG_JSON=true
LOG_TO_FILE=false
```

## Performance

### Overhead

- **Minimal impact**: < 1% performance overhead in production
- **Async-safe**: Uses context variables for correlation IDs
- **Efficient**: JSON serialization optimized with structlog

### Optimization Tips

- Use `LOG_LEVEL=WARNING` in high-traffic production
- Disable file logging (`LOG_TO_FILE=false`) for containerized deployments
- Configure log aggregation system sampling for very high volumes
