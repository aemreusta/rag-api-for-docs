classDiagram
    class CustomLLM {
        <<LlamaIndex>>
        +complete()
        +stream_complete()
    }
    
    class LLMProvider {
        <<Abstract>>
        +is_available() bool
        +complete(messages) CompletionResponse
        +stream_complete(messages) AsyncGenerator
    }
    
    class OpenRouterProvider {
        -timeout_seconds: int
        -model_name: str
        -logger: Logger
        +is_available() bool
        +complete(messages) CompletionResponse
        +stream_complete(messages) AsyncGenerator
    }
    
    class GroqProvider {
        -timeout_seconds: int
        -model_name: str
        -logger: Logger
        -redis_client: Any
        +is_available() bool
        +complete(messages) CompletionResponse
        +stream_complete(messages) AsyncGenerator
        +_parse_rate_limit_headers(headers) dict
        +_store_rate_limit_info(info) void
        +_get_stored_rate_limit_info() dict
        +_should_skip_due_to_rate_limits() tuple[bool, int]
        +_add_jitter(base_delay) float
        +_handle_successful_response(result) Any
        +_handle_timeout_error(attempt, max_retries, error) float
        +_handle_rate_limit_error(attempt, max_retries, error, error_str) float
        +_handle_service_error(attempt, max_retries, error_str) float
        +_check_rate_limit_preemption(attempt) void
        +_retry_with_backoff(func, *args) Any
        +_convert_messages_to_groq_format(messages) list
        +_create_completion_response(response) CompletionResponse
        +_create_streaming_response(chunk) CompletionResponse
    }
    
    class OpenAIProvider {
        -timeout_seconds: int
        -model_name: str
        -logger: Logger
        +is_available() bool
        +complete(messages) CompletionResponse
        +stream_complete(messages) AsyncGenerator
    }
    
    class LocalProvider {
        -timeout_seconds: int
        -model_name: str
        -logger: Logger
        +is_available() bool
        +complete(messages) CompletionResponse
        +stream_complete(messages) AsyncGenerator
    }
    
    class LLMRouter {
        -_redis_client: Any
        -_providers: list[LLMProvider]
        -_available_providers: list[LLMProvider]
        -_logger: Any
        +logger: Any
        +__init__()
        +redis_client: Any
        +available_providers: list[LLMProvider]
        +metadata: LLMMetadata
        +_get_cache_key(provider, model, error_type) str
        +_is_provider_cached_as_failed(provider, error_type) bool
        +_cache_provider_failure(provider, error_type)
        +_classify_error(error) ErrorType
        +_should_trigger_fallback(error) bool
        +_try_provider(provider, messages) CompletionResponse
        +acomplete(messages) CompletionResponse
        +complete(messages) CompletionResponse
        +astream_complete(messages) AsyncGenerator
        +stream_complete(messages) AsyncGenerator
    }
    
    CustomLLM <|-- LLMRouter
    LLMProvider <|-- OpenRouterProvider
    LLMProvider <|-- GroqProvider
    LLMProvider <|-- OpenAIProvider
    LLMProvider <|-- LocalProvider
    LLMRouter o-- LLMProvider 