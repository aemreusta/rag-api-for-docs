# Langfuse Evaluation Dataset Setup

## Overview

This document outlines the process for creating and managing evaluation datasets in Langfuse to establish quality baselines for our chatbot system.

## Task 3: Create Initial Evaluation Dataset

### Goal

Establish the "ground truth" for quality measurement. This dataset will be used to measure all future improvements and ensure consistent chatbot performance.

### Prerequisites

- Langfuse service running at <http://localhost:3000>
- API services up and running (`make up`)
- Access to sample policy documents

### Step-by-Step Process

#### 1. Access Langfuse UI

1. Navigate to <http://localhost:3000>
2. Log in to your Langfuse dashboard
3. Ensure you can see the main dashboard with traces from previous API calls

#### 2. Create New Dataset

1. Click on **"Datasets"** in the left sidebar
2. Click **"+ New dataset"** button
3. Fill in dataset details:
   - **Name**: `v1-policy-questions`
   - **Description**: `Initial evaluation dataset for charity policy chatbot v1.0`
   - **Tags**: Add relevant tags like `evaluation`, `v1`, `policy-qa`

#### 3. Add Evaluation Items

Add the following question/answer pairs to your dataset. Each item should include:

- **Input**: The user question
- **Expected Output**: The ideal concise answer
- **Metadata**: Category and difficulty level

### Recommended Question Categories

#### Category 1: Simple Fact-Finding (5-7 questions)

Questions that have direct answers in the policy documents.

**Example Questions:**

```
Q: What is the organization's policy on volunteer background checks?
Expected: All volunteers must complete a background check before starting their role. This includes criminal history and reference verification.

Q: How many hours per week are volunteers expected to commit?
Expected: Volunteers are expected to commit a minimum of 4 hours per week, with flexibility for scheduling based on program needs.

Q: What is the dress code for volunteers?
Expected: Volunteers should dress professionally and wear their provided ID badge at all times. Specific attire guidelines vary by department.
```

#### Category 2: Comparison Questions (3-5 questions)

Questions that require understanding relationships between different policies.

**Example Questions:**

```
Q: What's the difference between short-term and long-term volunteer commitments?
Expected: Short-term volunteers commit 1-3 months for specific projects, while long-term volunteers commit 6+ months for ongoing programs. Training requirements differ accordingly.

Q: How do volunteer benefits differ from staff benefits?
Expected: Volunteers receive recognition awards and training opportunities, while staff receive health insurance and paid time off. Both groups receive professional development support.
```

#### Category 3: Edge Cases & Unknown Information (3-5 questions)

Questions that should trigger "I don't know" responses when information isn't in the documents.

**Example Questions:**

```
Q: What is the organization's policy on cryptocurrency donations?
Expected: I don't have information about cryptocurrency donation policies in the available documents. Please contact our finance department for specific guidance.

Q: Can volunteers bring their pets to work?
Expected: I don't find specific information about pet policies for volunteers in the available documents. Please check with your supervisor or HR.
```

#### Category 4: Process & Procedure Questions (3-5 questions)

Questions about how to complete specific tasks or follow procedures.

**Example Questions:**

```
Q: How do I request time off as a volunteer?
Expected: Volunteers should submit time-off requests at least 2 weeks in advance through the volunteer coordinator. Use the online portal or email form provided during orientation.

Q: What should I do if I witness inappropriate behavior?
Expected: Report any inappropriate behavior immediately to your supervisor or use the anonymous reporting hotline. All reports are taken seriously and investigated promptly.
```

### Quality Guidelines for Answers

#### Good Expected Outputs Should

- Be concise (1-3 sentences typically)
- Directly answer the question asked
- Include specific details when available
- Use professional, helpful tone
- Acknowledge limitations when information isn't available

#### Avoid

- Overly lengthy responses
- Vague or generic answers
- Information not supported by source documents
- Contradictory statements

### Dataset Validation Checklist

Before finalizing your dataset, ensure:

- [ ] 15-20 question/answer pairs added
- [ ] Mix of question types (fact-finding, comparison, edge cases, processes)
- [ ] Expected outputs are concise and accurate
- [ ] Questions reflect real user scenarios
- [ ] "I don't know" responses included for out-of-scope questions
- [ ] Each item has appropriate metadata/tags

### Next Steps After Dataset Creation

1. **Baseline Evaluation**: Run initial evaluation to establish performance baseline
2. **Automated Testing**: Set up automated evaluation runs for regression testing
3. **Continuous Improvement**: Use results to identify improvement areas
4. **Dataset Evolution**: Regularly add new questions based on user feedback

### Evaluation Metrics to Track

- **Accuracy**: Percentage of correct answers
- **Completeness**: Coverage of source material in responses
- **Relevance**: How well answers address the specific question
- **Consistency**: Similar questions should get similar quality responses
- **Hallucination Rate**: Frequency of incorrect information not in source docs

### Documentation and Versioning

- Document all dataset changes in git commits
- Version datasets when making significant updates
- Track evaluation results over time
- Maintain changelog of improvements

## Manual Testing Verification

After creating the dataset:

1. Navigate to **"Evaluations"** in Langfuse UI
2. Click **"+ New evaluation"**
3. Configure the evaluation:
   - Select your `v1-policy-questions` dataset
   - Choose the `chat-request` generation template
   - Set up model configuration (manual testing for now)
4. Verify the dataset appears correctly and is ready for evaluation runs

## Success Criteria

✅ Dataset created with 15-20 diverse question/answer pairs
✅ Questions cover multiple categories (fact-finding, comparison, edge cases)
✅ Expected outputs are concise and accurate
✅ Dataset is visible in Langfuse Evaluations section
✅ Ready for baseline evaluation run

This evaluation dataset forms the foundation for measuring and improving our chatbot's performance over time.
