# Improved Ingest Flow Plan

## Executive Summary

This document outlines a comprehensive redesign of the chatbot-api-service ingestion pipeline to address current limitations and align with the project phases roadmap. The improved design focuses on scalability, reliability, and maintainability while supporting incremental processing and multi-format content.

## Current State Analysis

### Identified Issues with Existing Ingest Flow

1. **Fragmented Implementation**
   - Two separate scripts (`ingest.py`, `ingest_simple.py`) with duplicated logic
   - Inconsistent error handling and retry mechanisms
   - Manual coordination between scripts

2. **Limited Scalability**
   - No incremental updates - always full re-ingestion
   - Hardcoded document limits without proper validation
   - No background processing for large files

3. **Poor Document Management**
   - Manual document management with no API endpoints
   - No metadata tracking (versioning, authorship, tags)
   - No deduplication strategy for repeated content

4. **Restricted Format Support**
   - Only PDF processing currently supported
   - No URL scraping or web content ingestion
   - Limited to local filesystem storage

5. **Observability Gaps**
   - Basic logging without structured metrics
   - No ingestion progress tracking
   - Limited error diagnostics

## Improved Ingest Flow Architecture

### Phase 1: Core Restructuring & API Foundation

#### 1.1 Unified Ingestion Engine (`app/core/ingestion.py`)

Create a centralized ingestion engine with the following capabilities:

```python
class IngestionEngine:
    """Unified document ingestion and processing engine."""
    
    def __init__(self, storage_backend: StorageBackend, vector_store: PGVectorStore):
        self.storage = storage_backend
        self.vector_store = vector_store
        self.chunker = AdaptiveChunker()
        self.embedder = get_embedding_model()
    
    async def ingest_document(self, document: DocumentInput) -> IngestionResult:
        """Main ingestion pipeline with error handling and progress tracking."""
        
    async def process_url(self, url: str, options: ScrapingOptions) -> IngestionResult:
        """URL content extraction with robots.txt compliance."""
        
    def detect_duplicates(self, content_hash: str) -> Optional[Document]:
        """Content-based deduplication using SHA-256 hashes."""
        
    def generate_chunks(self, content: str, metadata: dict) -> List[DocumentChunk]:
        """Intelligent chunking with semantic boundaries."""
```

**Key Features:**

- Document validation & sanitization
- Format detection (PDF, DOCX, URL scraping)
- Chunking strategy with overlap optimization
- Embedding generation with retry logic
- Vector store operations with conflict resolution

#### 1.2 Document Management API

Implement comprehensive REST endpoints for document lifecycle management:

```bash
# Document Upload & Management
POST   /api/v1/docs/upload        # Multi-part file upload with validation
PUT    /api/v1/docs/{id}          # Document versioning & updates
POST   /api/v1/docs/scrape        # URL content extraction
GET    /api/v1/docs               # List documents with filters
GET    /api/v1/docs/{id}          # Get document metadata
GET    /api/v1/docs/status/{id}   # Ingestion progress tracking
DELETE /api/v1/docs/{id}          # Soft deletion with audit trail

# Batch Operations
POST   /api/v1/docs/batch/upload  # Multiple file upload
POST   /api/v1/docs/batch/scrape  # Multiple URL processing
GET    /api/v1/docs/batch/{job_id} # Batch job status

# Analytics & Monitoring
GET    /api/v1/docs/stats         # Ingestion statistics
GET    /api/v1/docs/health        # System health check
```

#### 1.3 Enhanced Database Schema

Implement comprehensive metadata tracking with the following tables (vector dimension aligned to 1536 and HNSW index enabled):

```sql
-- Documents metadata table
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255),
    content_hash VARCHAR(64) UNIQUE NOT NULL, -- SHA-256 for deduplication
    file_size BIGINT NOT NULL,
    mime_type VARCHAR(100) NOT NULL,
    page_count INTEGER,
    word_count INTEGER,
    language VARCHAR(10) DEFAULT 'en',
    uploaded_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    processed_at TIMESTAMP,
    author VARCHAR(100),
    tags TEXT[],
    status document_status DEFAULT 'pending',
    version INTEGER DEFAULT 1,
    storage_backend VARCHAR(20) DEFAULT 'local',
    storage_uri TEXT NOT NULL,
    soft_deleted BOOLEAN DEFAULT FALSE,
    created_by VARCHAR(100),
    metadata JSONB DEFAULT '{}',
    
    CONSTRAINT valid_file_size CHECK (file_size > 0 AND file_size <= 104857600), -- 100MB limit
    CONSTRAINT valid_version CHECK (version > 0)
);

-- Document processing status enum
CREATE TYPE document_status AS ENUM (
    'pending',     -- Uploaded, waiting for processing
    'processing',  -- Currently being ingested
    'completed',   -- Successfully processed
    'failed',      -- Processing failed
    'paused'       -- Processing paused (manual intervention required)
);

-- Document chunks with enhanced metadata
CREATE TABLE document_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    content_hash VARCHAR(64) NOT NULL, -- Chunk-level deduplication
    token_count INTEGER,
    chunk_type VARCHAR(50) DEFAULT 'paragraph', -- paragraph, table, header, etc.
    page_number INTEGER,
    section_title VARCHAR(255),
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(document_id, chunk_index),
    CONSTRAINT valid_chunk_index CHECK (chunk_index >= 0),
    CONSTRAINT valid_token_count CHECK (token_count > 0)
);

-- Processing jobs for async operations
CREATE TABLE processing_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_type VARCHAR(50) NOT NULL, -- 'upload', 'scrape', 'batch_upload', etc.
    status VARCHAR(20) DEFAULT 'pending',
    input_data JSONB NOT NULL,
    result_data JSONB,
    error_message TEXT,
    progress_percent INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    created_by VARCHAR(100),
    
    CONSTRAINT valid_progress CHECK (progress_percent >= 0 AND progress_percent <= 100)
);

-- Indexes for performance
CREATE INDEX idx_documents_content_hash ON documents(content_hash);
CREATE INDEX idx_documents_status ON documents(status) WHERE NOT soft_deleted;
CREATE INDEX idx_documents_tags ON documents USING GIN(tags);
CREATE INDEX idx_documents_created_at ON documents(uploaded_at DESC);
CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_content_hash ON document_chunks(content_hash);
CREATE INDEX idx_processing_jobs_status ON processing_jobs(status, created_at);
```

### Phase 2: Advanced Processing & Intelligence

#### 2.1 Smart Chunking Strategy

Implement intelligent content segmentation:

```python
class AdaptiveChunker:
    """Context-aware document chunking with sentence boundaries and overlap."""

    def chunk(self, text: str, *, max_tokens: int = 512, overlap: int = 50) -> list[str]:
        """Split text on sentence boundaries and pack into overlapping chunks."""
        ...
```

**Features:**

- **Semantic chunking** using sentence boundaries and topic coherence (default implementation uses NLTK-backed sentence tokenization with a graceful naive fallback when language resources are unavailable)
- **Adaptive chunk size** based on content type (legal docs vs. articles)
- **Overlap optimization** to maintain context across chunks
- **Metadata preservation** (headers, footnotes, tables)

#### 2.2 Incremental Processing Pipeline

Implement smart update detection and processing:

```python
class IncrementalProcessor:
    """Handles incremental updates and change detection."""
    
    def detect_changes(self, document_id: str, new_content_hash: str) -> ChangeSet:
        """Detect what has changed in a document update."""
        
    def process_incremental_update(self, document_id: str, changes: ChangeSet) -> None:
        """Process only the changed portions of a document."""
        
    def consolidate_embeddings(self, document_id: str) -> None:
        """Consolidate embeddings after incremental updates."""
```

**Capabilities:**

- Content-based deduplication using SHA-256 hashes
- Page-level change detection for PDF updates  
- Selective re-embedding of modified sections
- Version-aware conflict resolution

Current implementation status (Prototype):

- `app/core/incremental.py` provides `IncrementalProcessor` with:
  - `detect_changes` (hash-based detection; accepts optional per-page hashes)
  - `process_incremental_update` (deletes existing vectors for changed pages and re-embeds only those pages)
- Metadata extraction implemented:
  - `app/core/metadata.py` adds `ChunkMetadataExtractor` which detects header-like lines per page and derives a `section_title`.
  - Integrated in `IncrementalProcessor._prepare_chunk_inputs` so chunks are upserted with `page_number`, `section_title`, and `extra_metadata.headers`.
  - Background job path annotates chunks with empty headers when page context is unavailable; to be enhanced when page splits are available.
- Endpoints exposed under `api/v1/docs`:
  - `POST /api/v1/docs/detect-changes`
  - `PUT /api/v1/docs/apply-changes`
- Tests added in `tests/test_incremental_processor.py` for detection and selective re-embedding (integration test marked with `@pytest.mark.integration`).

#### 2.3 Multi-Format Support

Extend format support beyond PDFs:

```python
class FormatProcessor:
    """Handles multiple document formats with consistent output."""
    
    def process_pdf(self, file_path: str) -> ProcessedDocument:
        """Enhanced PDF processing with table and image extraction."""
        
    def process_docx(self, file_path: str) -> ProcessedDocument:
        """DOCX processing with style and formatting preservation."""
        
    def process_url(self, url: str, options: ScrapingOptions) -> ProcessedDocument:
        """Web content extraction with robots.txt compliance."""
        
    def sanitize_html(self, html_content: str) -> str:
        """Clean and sanitize HTML content for safe processing."""
```

### Phase 3: Production & Monitoring

#### 3.1 Background Processing

Implement async job processing with proper queueing:

```python
# Redis-backed job queue for scalable processing
from celery import Celery

app = Celery('ingestion_worker')

@app.task(bind=True, max_retries=3)
def process_document_async(self, job_id: str, document_data: dict):
    """Background document processing with retry logic."""
    
@app.task(bind=True, max_retries=5, retry_backoff=True)
def scrape_url_async(self, job_id: str, url: str, options: dict):
    """Background URL scraping with exponential backoff."""
    
@app.task
def cleanup_failed_jobs():
    """Periodic cleanup of failed processing jobs."""
```

**Features:**

- Non-blocking file uploads with progress tracking
- Batch processing for multiple documents
- Retry logic with exponential backoff
- Resource throttling to prevent system overload

#### 3.2 Quality Assurance

Implement comprehensive validation and quality checks:

```python
class QualityAssurance:
    """Comprehensive document and processing quality validation."""
    
    def validate_upload(self, file_data: bytes, metadata: dict) -> ValidationResult:
        """Pre-processing validation of uploaded files."""
        
    def score_content_quality(self, content: str) -> QualityScore:
        """Assess content quality and readability."""
        
    def validate_embeddings(self, embeddings: List[float]) -> bool:
        """Ensure embedding consistency and validity."""
        
    def check_vector_store_integrity(self) -> IntegrityReport:
        """Verify vector store consistency and performance."""
```

#### 3.3 Monitoring & Observability

Enhanced monitoring with structured metrics (Implemented):

```python
# Metrics collection
class IngestionMetrics:
    """Comprehensive metrics collection for ingestion pipeline."""

    def record_ingest_request(self, *, status: str, duration_seconds: float | None = None) -> None:
        ...

    def record_embedding_latency(self, *, stage: str, duration_seconds: float) -> None:
        ...

    def record_job_duration(self, *, job_type: str, status: str, duration_seconds: float) -> None:
        ...

    def increment_job_counter(self, *, job_type: str, status: str) -> None:
        ...

    def increment_pages_processed(self, *, status: str, count: int = 1) -> None:
        ...

    def increment_chunks_processed(self, *, status: str, count: int = 1) -> None:
        ...

# Structured logging with correlation IDs (Implemented)
logger.info(
    "Document processed successfully",
    extra={
        "document_id": doc_id,
        "processing_time_ms": duration,
        "chunk_count": len(chunks),
        "storage_backend": backend,
    "trace_id": trace_id
    }
)
```

### Phase 4: Scale & Security

#### 4.1 Storage Backends

Implement pluggable storage architecture:

```python
class StorageBackend(ABC):
    """Abstract base class for storage backends."""
    
    @abstractmethod
    async def store_file(self, file_data: bytes, path: str) -> StorageResult:
        """Store file data and return storage URI."""
        
    @abstractmethod
    async def retrieve_file(self, storage_uri: str) -> bytes:
        """Retrieve file data from storage URI."""

class LocalStorageBackend(StorageBackend):
    """Local filesystem storage for development."""

class MinIOStorageBackend(StorageBackend):
    """S3-compatible storage for production scalability."""
    
class CloudStorageBackend(StorageBackend):
    """Cloud provider storage (AWS S3, GCP Storage, etc.)."""
```

#### 4.2 Security & Compliance

Implement comprehensive security measures:

```python
class SecurityValidator:
    """Security validation and threat detection."""
    
    def scan_for_malware(self, file_data: bytes) -> ScanResult:
        """Scan uploaded files for malicious content."""
        
    def detect_pii(self, content: str) -> PIIDetectionResult:
        """Detect and flag personally identifiable information."""
        
    def sanitize_content(self, content: str) -> str:
        """Remove or redact sensitive information."""
        
    def validate_access_permissions(self, user_id: str, doc_id: str) -> bool:
        """Check document-level access permissions."""
```

## Implementation Roadmap

### Sprint 1: Foundation (2 weeks)

- [x] Create unified ingestion engine (`app/core/ingestion.py`) — branch `feat/core-ingestion-engine` merged into `develop` in commit f36eed6
- [x] Implement basic document management API endpoints — branch `feat/api-docs-endpoints` (in-memory prototype for upload/list/get/status/scrape with tests)
- [x] Design and implement enhanced database schema — `content_embeddings.content_vector VECTOR(1536)` with HNSW index
- [x] Set up background job processing with Celery/Redis
- [x] Basic upload validation and error handling

### Sprint 2: Intelligence (2 weeks)  

- [ ] Implement adaptive chunking strategy
- [ ] Add multi-format support (PDF, DOCX, URLs)
- [x] Build incremental processing pipeline
- [ ] Content deduplication and version management
  - [x] Document-level SHA-256 deduplication and version bumping via `ContentDeduplicator`
  - [x] Chunk-level SHA-256 deduplication and selective updates
- [x] Enhanced metadata extraction (initial: headers/section_title per page)

### Next Steps (detailed)

1) [x] Persist uploads and dedup into DB and storage

- Wire `POST /api/v1/docs/upload` to:
  - Store file to `uploaded_docs/` (or active storage backend) with safe path
  - Compute SHA-256 content hash and call `ContentDeduplicator.upsert_document_by_hash`
  - Return DB `document_id` and initial status
- Acceptance: uploading the same file returns existing doc; modified file bumps version.

1) [x] Full incremental update flow

- `PUT /api/v1/docs/apply-changes` currently re-embeds changed pages and bumps version via `IncrementalProcessor` → `ContentDeduplicator`.
- Extend to also upsert chunk rows using `ContentDeduplicator.upsert_chunks` with chunk indices and metadata.
- Acceptance: changed pages update rows; unchanged stay intact; row counts only change for inserts.

1) [x] Chunk metadata extraction

- Implement `ChunkMetadataExtractor` with conservative heuristics for header detection and page-level `section_title` derivation.
- Wire into incremental processing so all changed-page chunks carry `page_number`, `section_title`, and `headers` in `extra_metadata`.
- Update background job path to emit the same metadata fields with empty headers when page context is unavailable.
- Acceptance: chunk rows populated with metadata; logs show detected headers summary per page.

1) [x] Background processing (Celery/Redis)

- Offload heavy ingest and re-embedding to Celery workers.
- Expose job status in `/api/v1/docs/jobs/{job_id}` and batch job endpoints.
- Acceptance: API responds fast; long tasks tracked with progress.

1) [x] Observability & metrics

- Add counters/timers for: dedup hits, version bumps, chunk inserts/updates, embedding latency. (Implemented)
- Implement `IngestionMetrics` for ingest request/job/page/chunk metrics. (Implemented)
- Hook into structured logs with correlation IDs in requests and worker jobs. (Implemented)
- Acceptance: Prometheus metrics exposed; logs include `request_id` and `trace_id`; dashboards show ingest/job trends. (Implemented)

1) Security & validation

- Enforce file size/type limits; sanitize filenames; reject malformed payloads.
- Redact secrets in logs; do not log file contents.
- Acceptance: tests cover invalid types, oversize files, missing fields.

1) Tests

- Unit: `ContentDeduplicator` edge cases (conflicting URIs, empty chunks, duplicate indices).
- Integration: upload → dedup → chunk upsert → re-embed → query vector store.
- API: contract tests for new/updated endpoints and error paths.

1) Documentation & diagrams

- Update Mermaid diagrams in `docs/diagrams/` for dedup/versioning and incremental flows.
- Add API docs for new/updated endpoints with request/response examples.

### Sprint 3: Production Ready (1 week)

- [ ] Add comprehensive monitoring and metrics
- [ ] Implement quality assurance checks
- [ ] Storage backend abstraction (local, MinIO)
- [ ] Performance optimization and caching
- [ ] Complete API documentation

### Sprint 4: Security & Scale (1 week)

- [ ] Security validation and malware scanning
- [ ] Access control and audit logging
- [ ] PII detection and redaction
- [ ] Load testing and performance validation
- [ ] Production deployment guides

## Success Metrics

### Performance Targets

- **Ingestion Latency**: < 30 seconds per MB of content
- **API Response Time**: < 200ms for metadata operations
- **Background Job Processing**: 95% success rate
- **Storage Efficiency**: 90% deduplication rate for repeated content

### Embeddings & Retrieval Configuration

- **Embedding Provider**: Google AI Studio
- **Embedding Model**: `gemini-embedding-001`
- **Embedding Dimension**: 1536
- **pgvector Index**: HNSW (`m=32`, `ef_construction=64`) using `vector_l2_ops`

### Quality Targets  

- **Content Accuracy**: 99% chunk integrity validation
- **Format Support**: PDF, DOCX, URL scraping
- **Error Recovery**: Automatic retry for transient failures
- **Monitoring Coverage**: 100% operation traceability

### Scalability Targets

- **Concurrent Uploads**: Support 100+ simultaneous uploads
- **Document Volume**: Handle 10,000+ documents efficiently
- **Storage Backends**: Seamless migration between storage types
- **API Throughput**: 1,000+ requests per minute

## Migration Strategy

### Phase 1: Parallel Deployment

1. Deploy new ingestion API alongside existing scripts
2. Migrate existing documents to new metadata schema
3. Run both systems in parallel for validation
4. Gradual traffic shift to new system

### Phase 2: Legacy Cleanup

1. Deprecate old ingestion scripts with clear migration path
2. Update CI/CD pipelines to use new API endpoints
3. Remove legacy code after successful migration
4. Update documentation and developer guides

## Risk Mitigation

### Technical Risks

- **Database Migration Complexity**: Implement comprehensive rollback procedures
- **Storage Migration**: Dual-write strategy during transition
- **Performance Regression**: Extensive load testing before deployment
- **API Compatibility**: Versioned APIs with deprecation timeline

### Operational Risks

- **Data Loss Prevention**: Multiple backup strategies and validation
- **Service Availability**: Zero-downtime deployment procedures
- **Monitoring Gaps**: Comprehensive alerting for all critical paths
- **Security Vulnerabilities**: Regular security audits and penetration testing

## Conclusion

This improved ingest flow plan addresses all current limitations while providing a scalable foundation for future growth. The phased approach ensures minimal disruption while delivering immediate value through better reliability, observability, and user experience.

The architecture emphasizes modularity, making it easy to extend support for new formats, storage backends, and processing capabilities as requirements evolve.

---

**Document Version**: 1.0  
**Last Updated**: 2025-08-13  
**Status**: In Progress — chat flow stabilized; `.env.example` updated for Gemini embeddings (1536); document endpoints and incremental processing endpoints implemented; initial chunk metadata extraction (headers/section_title) integrated; tests passing
