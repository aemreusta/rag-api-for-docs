# Tech Stack Document

This document outlines the "best-of-breed" technologies for the Charity Policy Chatbot API.

| Component | Technology | Rationale / Purpose |
|-----------|------------|-------------------|
| Backend Framework | FastAPI | High-performance Python framework. Remains the best choice for the API layer due to its speed and Pydantic integration. |
| LLM App Framework | LlamaIndex | (Upgrade) Replaces custom RAG logic. A specialized data-centric framework providing SOTA components for ingestion, indexing, and advanced retrieval to maximize answer quality. |
| Vector Database | PostgreSQL w/ pgvector + HNSW | High-performance vector similarity search with P99 <2ms latency. HNSW index (m=32, ef_construction=64) provides 27× better performance than target. |
| LLM Observability | Langfuse | (Upgrade) Replaces manual tracking. A purpose-built platform for tracing, debugging, evaluating, and monitoring LLM applications. Solves the "black box" problem and provides key metrics out-of-the-box. |
| Metrics & Monitoring | Prometheus/DataDog/OpenTelemetry/NoOp | Flexible multi-backend observability with auto-detection. Supports seamless migration between monitoring providers without code changes. |
| Structured Logging | structlog + python-json-logger | JSON-formatted logs with correlation IDs, sensitive data masking, and request/response tracing. Compatible with ELK, Loki, and OTEL collectors for production observability. |
| LLM Provider Routing | Multi-Provider Router (OpenRouter/Groq/OpenAI/Local) | Production-ready priority-based routing with automatic fallback, Redis caching, and comprehensive error handling. Ensures high availability across multiple LLM providers. |
| Language | Python 3.10+ | The required language for the entire modern data and AI stack. |
| In-Memory Datastore | Redis | A high-speed key-value store. Used for rate limiting, session management, conversational memory, and provider failure caching. |
| AI Model Access | OpenRouter + Multi-Provider | Provides model flexibility and cost comparison. Integrated with automatic fallback to Groq, OpenAI, and local models for high availability. |
| Containerization | Docker | Used to package the FastAPI app and its dependencies into a portable container for consistent deployment. |

## Performance Achievements ✅

| Component | Achievement | Impact |
|-----------|-------------|---------|
| pgvector HNSW | P99: 1.88ms latency | 27× faster than target (<50ms) |
| Vector Search Recall | 100% accuracy | Perfect recall at top-1, top-5, top-10 |
| Test Coverage | 95 tests passing | Zero warnings, comprehensive coverage |
| Monitoring Flexibility | 4 backend support | Zero vendor lock-in, easy migration |
| LLM Router Reliability | Automatic fallback | High availability across providers |

## Architecture Highlights

### High-Performance Vector Search

- HNSW Index: Hierarchical Navigable Small World algorithm
- Optimized Parameters: m=32, ef_construction=64, ef_search=100
- Embedding Dimension: 1536 (configurable via EMBEDDING_DIM)
- Performance: Average 1.3ms, P99 1.88ms

### Flexible Monitoring

- Auto-Detection: Automatically selects best available backend
- Zero Migration Cost: Change backends via environment variables
- Production-Ready: Comprehensive metrics for vector search, HTTP, and LLM providers
- Development-Friendly: NoOp backend for zero overhead testing

### Robust LLM Integration

- Multi-Provider Support: OpenRouter (Gemini) → Groq (Llama3) → OpenAI (GPT) → Local
- Intelligent Fallback: Automatic error detection and provider switching
- Caching Strategy: Redis-based failure caching with 5-minute cooldown
- Comprehensive Logging: Structured logs with correlation IDs

This technology stack provides production-grade performance, high availability, and operational flexibility while maintaining development simplicity.
