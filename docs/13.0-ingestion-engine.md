# Ingestion Engine (skeleton)

Purpose:

- Provide a pluggable ingestion pipeline with clean interfaces for parsing and chunking
- Emit progress callbacks for observability without coupling to specific backends

Key pieces:

- `FormatProcessor`: converts file input into raw text (`supports()`, `process()`)
- `AdaptiveChunker`: splits text into chunks (`chunk()`)
- `IngestionEngine`: orchestrates, validates, and emits progress events (`started`, `processed`, `chunked`, `completed`, `failed`)

Next steps:

- Implement concrete processors for PDF/DOCX/URL
- Replace the simple test chunker with an adaptive semantic chunker
- Connect metrics/logging hooks where needed and update Mermaid diagrams if architecture changes

Implemented (current):

- Incremental processing skeleton via `IncrementalProcessor` in `app/core/incremental.py`
  - `detect_changes(db, document_id, new_content_hash, page_hashes)` returns changed pages and reason
  - `process_incremental_update(db, document_id, changes, new_content_hash, page_texts)` selectively re-embeds changed pages
- Public API endpoints (prototype) added under `api/v1/docs`:
  - POST `/api/v1/docs/detect-changes` → returns `{ is_new_version, changed_pages, reason }`
  - PUT `/api/v1/docs/apply-changes` → re-embeds changed pages and returns `{ updated_pages, version }`
