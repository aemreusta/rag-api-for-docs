# Project Phases

This updated plan leverages the new frameworks to accelerate development and focus on quality.

## Phase 1: Setup & Data Ingestion ✅ **COMPLETED**

**Goal**: Establish the core infrastructure and create a high-quality, indexed knowledge base.

**Details**:

- ✅ **Setup**: Configure the environment with FastAPI, LlamaIndex, Langfuse, Docker, Postgres, and Redis.
- ✅ **Ingestion Script**: Refactor `scripts/ingest.py` to use LlamaIndex's PyMuPDFReader and SentenceSplitter (or a more advanced node parser).
- ✅ **Indexing**: Configure the PGVectorStore in LlamaIndex and run the ingestion script to populate the database with embeddings.
- ✅ **Observability**: Integrate the Langfuse SDK from day one to trace the ingestion process.
- ✅ **Quality Assurance**: Comprehensive testing (7/7 tests passing), pre-commit hooks, and code quality validation.

**Outcome**: ✅ **ACHIEVED** - A versioned, indexed knowledge base in PostgreSQL and a repeatable ingestion script.

**Current Status**:

- API server running on <http://localhost:8000>
- PostgreSQL with pgvector extension enabled
- Redis cache operational
- Langfuse observability platform running on <http://localhost:3000>
- PDF documents successfully indexed (sample_policy.pdf)
- All tests passing (35% code coverage)
- Pre-commit hooks and quality gates implemented

## Phase 2: Query Engine Development & Evaluation ✅ **COMPLETED**

**Goal**: Build and validate the core RAG pipeline that can answer questions accurately.

### Todo List

- ✅ **Build Core Query Engine**: Implement `core/query_engine.py` with:
  - ✅ Initialize PGVectorStore with content_embeddings table
  - ✅ Configure OpenRouter LLM integration
  - ✅ Set up VectorStoreIndex
  - ✅ Implement CondenseQuestionChatEngine for conversational context
  - ✅ Expose get_chat_response function
- ✅ **API Endpoint**: Wire the query engine to the FastAPI endpoint in `chat.py`.
- ✅ **Create Dataset**: In Langfuse, create a "golden dataset" of 20-50 important questions with ideal answers.
- ✅ **Evaluate**: Run your first evaluations using Langfuse to score the baseline engine on metrics like Faithfulness, Answer Relevancy, and Context Precision.

**Outcome**: ✅ **ACHIEVED** - A functional API endpoint whose quality is tracked and measured.

**Completed Tasks**:

1. ✅ ~~Implement chat endpoint in `app/api/v1/chat.py`~~
2. ✅ ~~Wire query engine to FastAPI routes~~
3. ✅ ~~Create evaluation dataset in Langfuse~~
4. ✅ ~~Run initial quality assessments~~

**Current Status**:

- Chat API endpoint functional with authentication
- Query engine integrated with OpenRouter LLM
- Baseline evaluation script implemented
- Test coverage for chat functionality
- System ready for Phase 3 development

## Phase 3: Advanced Features & Deployment ⏳ **PLANNED**

**Goal**: Implement conversational memory and deploy the secure, observable service.

**Details**:

- ✅ **Conversational Memory**: Upgrade the QueryEngine to a ChatEngine using LlamaIndex's built-in memory components to handle follow-up questions.
- **Security & Rate Limiting**: Implement the API key authentication and Redis-based rate limiting. These remain outside the LlamaIndex logic.
- **Containerize & Deploy**: Finalize the Dockerfile and docker-compose.yml (including the Langfuse container if self-hosting) and deploy the stack to a cloud provider.
- **Integrate**: Work with the frontend team, providing the API spec and explaining how to capture user feedback scores to send to the Langfuse API.

**Outcome**: A smart, secure, and observable chatbot is live and accessible to the public.

## Phase 4: Continuous Improvement & Tuning ⏳ **PLANNED**

**Goal**: Use the observability platform to systematically improve the chatbot's performance, cost, and quality.

**Details**:

- **Monitor**: Actively monitor the Langfuse dashboards for cost spikes, latency degradation, or high rates of low-scoring user feedback.
- **Debug**: Use the detailed traces in Langfuse to analyze and fix any user-reported issues or bad responses.
- **A/B Test**: Use the Langfuse evaluation tools to A/B test different prompts, LLMs (via OpenRouter), or LlamaIndex retriever settings to find the optimal configuration.
- **Tune**: Based on data, make informed decisions to tune the system for better performance and user satisfaction.

**Outcome**: A mature, data-driven workflow for maintaining and enhancing a high-quality AI service.

## Development Commands for Current Phase

With Phase 1 complete and Phase 2 in progress, these are the key commands for ongoing development:

```bash
# System Management
make up          # Start all services
make down        # Stop all services
make logs        # View service logs

# Development
make test        # Run tests (should show 7/7 passing)
make lint        # Check code quality
make format      # Format code
make shell       # Access app container

# Database
make db-shell    # Access PostgreSQL
make ingest      # Re-run data ingestion
```

## Quality Metrics Achieved

- ✅ **Tests**: 7/7 passing
- ✅ **Coverage**: 35% (appropriate for infrastructure phase)
- ✅ **Linting**: All checks pass
- ✅ **Security**: Secrets scanning enabled
- ✅ **Documentation**: Comprehensive setup guides
- ✅ **Observability**: Langfuse integration ready
- ✅ **Evaluation Dataset**: v1-policy-questions dataset created
