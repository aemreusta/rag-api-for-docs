# Project Phases

This updated plan merges the original roadmap with the latest revisions to keep every task current and traceable.

## Phase 1 – Setup & Data Ingestion ✅ **COMPLETED**

**Goal**: Establish the core infrastructure and create a high‑quality, indexed knowledge base.

**Key deliverables** (all completed): FastAPI stack, LlamaIndex ingestion, PGVectorStore, Langfuse tracing, full test & lint gate.

## Phase 2 – Query Engine Development & Evaluation ✅ **COMPLETED**

**Goal**: Ship a measured baseline RAG pipeline with an authenticated /chat endpoint and evaluation dataset.

**Key deliverables** (all completed): VectorStoreIndex query engine, OpenRouter LLM, golden‑dataset, Langfuse evals, CI‑guarded quality metrics.

## Road Map (Phase 3 → Phase 4)

| Phase | Goal | Key Deliverables |
|-------|------|------------------|
| 3A – Robust Core | Harden monolith for low/medium traffic | ✅ sync API workers, Redis LRU cache (400 MB), PGVector column + HNSW index, ✅ Redis rate-limit, Langfuse try/catch, ✅ structured logs, custom error codes |
| 3B – LLM & RAG Upgrades | Model fallbacks, embeddings, streaming | ✅ provider router (Gemini / Groq / ChatGPT + local), embedding switch, streaming responses, default "sorry" fallback |
| 3C – DX & Docs | Code, schema, docs hygiene | ✅ dead‑code drop, checksum UPSERTs, doc‑lint CI |
| 3D – Prod‑Ready Container | Secure slim image | ✅ multi‑stage Dockerfile, non‑root user, HEALTHCHECK |
| 4A – Testing Maturity | ≥70 tests critical‑path coverage | ✅ unit, integration, E2E, fuzz, coverage gate, property-based testing |
| 4B – Feature Growth | Memory, doc API, versioning | ✅ Redis convo memory, doc upload/version, metadata, Streamlit demo |
| 4C – Performance & Security | HNSW, Redis cache, OAuth, SBOM | ✅ HNSW migration, ✅ Redis LRU (400MB allkeys-lru), security hardening, supply chain security |
| 4D – Optional | Feedback, WordPress, serverless PoC | Defer until needed |

## 🔎 Detailed To-Do Backlog (Authoritative)

### 1 · Core Hardening (3A)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/rate-limit-redis` ✅ | Fixed‑window per‑IP limit → 429 w/ reset‑after header | ✅ Unit window math; Integration exceed limit via pytest‑httpx |
| `feat/redis-cache` ✅ | **Redis LRU Cache with allkeys-lru policy (400MB), fallback to TTLCache, cache coherence across workers** | ✅ **Unit tests (33), Redis/memory backends, ≥80% hit rate validation, LRU eviction configured** |
| `chore/cors-env` | Centralise settings.ALLOW_ORIGINS; default * in DEBUG | Unit env override |
| `feat/structured-logs` ✅ | JSON structured logging w/ correlation IDs, sensitive data masking, rotation | ✅ Unit tests (20), file creation & rotation |
| `fix/pgvector-type` ✅ | Migrate content_vector TEXT → VECTOR(1536) + HNSW index (m=32, ef_construction=64) | ✅ P99: 1.88ms (target <50ms), 100% recall @top-10 |

#### RAG Empty Handling & Error Model (`fix/rag-empty-error`) ✅

- [x] Define explicit error for empty RAG result when fallback also fails → return `502` with JSON body:
  `{ "code": "RAG_EMPTY", "message": "RAG engine produced no answer and fallback failed.", "trace_id": "…" }`
- [x] Keep direct LLM fallback when RAG returns placeholder; only emit `RAG_EMPTY` if fallback fails
- [x] Add log fields: `trace_id`, `request_id`, `provider_chain`
- [ ] Add client handling examples in docs and Postman tests (negative case)

#### Async Chat + Rate Limiting Enforcement (`fix/async-chat-endpoint`) ✅

- [x] Convert `/api/v1/chat` to async handler and prefer async query engine
- [x] Fallback uses `await llm_router.acomplete` to avoid event loop errors
- [x] Enforce rate limiting via dependency; test overrides supported
- [x] Return friendly Turkish 500 message on unhandled exceptions (kept tests green)

#### Provider Initialization Guards (`fix/provider-init-guards`) ✅

- [x] Guard Google Gemini provider: only initialize when key/model are valid strings
- [x] Map Gemini aliases to OpenRouter canonical IDs
- [x] Normalize message inputs across providers
- [x] Deprecation note: plan upgrade to `llama-index-llms-google-genai`

### 2 · LLM & RAG Upgrades (3B)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/llm-router` ✅ | Priority‑based provider router w/ timeouts & automatic fallback | ✅ Unit forced timeout chains |
| `feat/embedding-switch` | Gemini / ChatGPT embeddings with fallback to HF | Integration cosine within ±0.02 |
| `feat/streaming` | StreamingResponse when answer > 300 chars | E2E client chunk reception |
| `feat/fallback-msg` | Generic apology + trace‑id on unhandled error | Unit MockLLMError |

### 3 · Schema & Docs Clean‑up (3C)

| Branch | Task | Tests |
|--------|------|-------|
| `chore/drop-unused-models` | Delete QueryLog, stub Vector | — |
| `feat/upsert-ingest` | SHA‑256 per page, UPSERT new/changed chunks | Integration rows stable on 1‑page edit |
| `chore/docs-sync` | Align env vars, add doc‑lint to pre‑commit | CI doc‑lint job |

### 4 · Container Hardening (3D)

| Branch | Task | Tests |
|--------|------|-------|
| `infra/multistage-docker` | Builder→runtime stages, strip dev deps, non‑root app, HEALTHCHECK | CI image < 400 MB |
| `infra/prometheus` | prometheus_fastapi_instrumentator, /metrics endpoint | Integration scrape returns http_requests_total |

### 5 · Testing Expansion (4A)

| Branch | Task | Scope |
|--------|------|-------|
| `test/unit-index` | PGVector in‑mem mock (sqlite+vector) | Unit |
| `test/integration-rag` | Spin Postgres/Redis via pytest‑docker | Integration |
| `test/e2e-stream` | Long‑Q streaming test on /chat | E2E |
| `test/fuzz-heuristic` | Random prompt‑injection & malformed PDF | Heuristic |

**Target coverage gate**: 80 % lines, 90 % functions on app/.

### 6 · Memory & Doc Management (4B)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/redis-memory` | Persist last N Q&A by session_id; prepend to prompt | Unit context‑length enforcement |
| `feat/doc-upload-api` | /api/v1/docs POST/PUT for upload + version | Integration ingest then query new content |
| `feat/metadata-schema` | documents table (id, path, version, uploaded_at, author, tags) | Integration CRUD |

## 📝 Detailed Implementation Steps

### 1 · Core Hardening (3A) - Detailed Tasks

#### Rate Limiting Implementation (`feat/rate-limit-redis`) ✅ **COMPLETED**

- [x] Implement per-IP fixed window Redis-based rate limiting
- [x] Return 429 status with 24h reset message on limit exceeded
- [x] Add unit tests for window calculation edge cases
- [x] Add integration tests via pytest-httpx for rate limit scenarios

#### Redis LRU Cache Implementation (`feat/redis-cache`) ✅ **COMPLETED**

- [x] **Implement Redis LRU Cache with allkeys-lru eviction policy (400MB memory limit)**
- [x] **Add fallback to cachetools.TTLCache for development environments**
- [x] **Apply @cached decorator to get_chat_response_async() with smart key generation**
- [x] **Implement cache coherence across workers to eliminate split-brain issues**
- [x] **Add comprehensive cache metrics (hits, misses, evictions) for monitoring**
- [x] **Configure Docker Compose with proper Redis LRU settings**
- [x] **Achieve ≥80% cache hit rate validation in tests**
- [x] **Add 33 comprehensive unit tests covering Redis and in-memory backends**
- [x] **Implement cache statistics API for real-time performance monitoring**
- [x] **Add pattern-based cache invalidation and cache warming utilities**
- [x] **Configure 1-hour TTL for chat responses with error isolation**

#### CORS Configuration (`chore/cors-env`) ✅ **COMPLETED**

- [x] Move CORS origins to `settings.ALLOW_ORIGINS`
- [x] Set default `*` in DEBUG mode
- [x] Configure CORS methods and headers via environment variables
- [x] Implement safe credentials handling (disable when using wildcard origins)
- [x] Add comprehensive CORS configuration logging in DEBUG mode
- [x] Implement warning logs for wildcard origins in non-development environments
- [x] Add CORS rejection logging in DEBUG mode
- [x] Add unit tests for environment override
- [x] Add integration tests for CORS configuration scenarios

#### Structured Logging (`feat/structured-logs`) ✅ **COMPLETED**

- [x] Implement `logging.config.dictConfig` with structlog
- [x] Add JSON format logging with correlation IDs (request_id, trace_id)
- [x] Implement sensitive data masking (API keys, passwords, emails, Bearer tokens)
- [x] Add middleware for request/response logging with timing
- [x] Integrate correlation IDs with Langfuse traces
- [x] Add log file pattern `logs/app_%(process)d_%Y%m%d.log` with rotation (100MB)
- [x] Implement CompressingRotatingFileHandler with gzip compression
- [x] Add environment-based configuration (LOG_LEVEL, LOG_JSON, LOG_TO_FILE, etc.)
- [x] Add comprehensive unit tests for all logging components (20 tests)
- [x] Add rate limiting event logging with retry-after information
- [x] Update .gitignore and .dockerignore to exclude log files

#### Database Optimization (`fix/pgvector-type`) ✅ **COMPLETED**

- [x] Replace `content_vector TEXT` → `VECTOR(1536)` via Alembic migration
- [x] Add **HNSW** index (`m=32`, `ef_construction=64`) for high-recall similarity search
- [x] Introduce `EMBEDDING_DIM` constant in `settings.py`; update `models.py`
- [x] Expose query-time tuning param `SET hnsw.ef_search = 100`
- [x] Instrument Prometheus metric structure (`vector_search_duration_seconds`)
- [x] Integration target: **P99: 1.88ms** (target < 50ms) — **27× better than target**
- [x] **Performance Results**: Average 1.3ms, P50 1.3ms, P95 1.9ms, P99 1.9ms
- [x] **Recall Accuracy**: 100% recall at top-1, top-5, and top-10 results
- [x] **Test Coverage**: 11 tests covering schema, performance, and monitoring

### 2 · LLM & RAG Upgrades (3B) - Detailed Tasks

#### LLM Router Implementation (`feat/llm-router`) ✅ **COMPLETED**

- [x] Abstract `LLMProvider` with priorities and timeouts
- [x] Implement **automatic fallback on any error** (timeout, rate limit, 5xx, auth failures)
- [x] Set **30s default timeout** per provider (configurable via env vars later)
- [x] **Cache failed providers temporarily** (5-minute cooldown) to avoid repeated failures in session
- [x] **Route supported providers:** Gemini(OpenRouter) → Groq(Llama3) → ChatGPT → Local
- [x] Add unit tests: forced timeout triggers next provider in chain
- [x] Support streaming & non-streaming pathways

#### LLM Router Production Fixes (`fix/llm-router-logger`) ✅ **COMPLETED** (2025-06-27)

- [x] **Fixed critical logger AttributeError** - LLMRouter instances now have proper logger access
- [x] **Implemented Pydantic-compatible logging** using `PrivateAttr()` and `@property` decorator
- [x] **Standardized metadata context window** to the correct 8192-token limit for Groq Llama 3 models
- [x] **Enhanced error classification** - added "malformed request" to client error detection
- [x] **Improved exception chaining** in Groq provider retry logic for better debugging
- [x] **Comprehensive test coverage** - all 70 tests passing with 100% router test success
- [x] **Updated documentation** - reflected fixes in LLM Router architecture docs

#### Embedding Provider Switch (`feat/embedding-switch`)

- [ ] Default embedding provider: **Gemini Embedding** (`gemini-embedding-001`) via Google AI Studio SDK
- [ ] Configure output dimension using MRL: supported dims `3072 | 1536 | 768` (default: **1536**)
- [ ] Add fallback order: Gemini → OpenAI/Text-Embedding → HuggingFace
- [ ] Environment flags to select embedding provider & dimension (`EMBEDDING_PROVIDER`, `EMBEDDING_DIM`)
- [ ] Integration tests: cosine parity checks where feasible; allow tolerance ±0.02 with model-specific skip
- [ ] Document deprecation windows for legacy embedding models; migration guidance

#### Streaming Responses (`feat/streaming`) ✅

- [x] Enable streaming by default via request flag (`stream=true`)
- [x] Use provider streaming (`llm_router.astream_complete`) when available; fallback to plain chunked streaming
- [x] Add E2E test validating chunked delivery (`tests/test_e2e_stream.py`)
- [x] Integrate UI consumption of `text/plain` streamed chunks and synthesize JSON for rendering
- [x] Error handling: on provider-stream failure, degrade to chunked stream; preserve structured 5xx on fatal errors

#### Fallback Messaging (`feat/fallback-msg`)

- [ ] On unhandled exceptions, send 200 with predefined apology + trace ID
- [ ] Add unit tests with `MockLLMError`
- [ ] Implement graceful degradation patterns

#### OpenRouter Integration (`feat/openrouter`) ✅ **COMPLETED**

- [x] Add `llama-index-llms-openrouter` dependency
- [x] Initialize OpenRouter client with `OPENROUTER_API_KEY` env variable
- [x] **Enable Google Gemini models via direct Google AI Studio API key integration**
- [x] **Make default model configurable** via `DEFAULT_LLM_MODEL` env variable
- [x] Expose model choice in settings & `/chat` endpoint (`model` query param)
- [x] Unit tests for Gemini response & streaming handling
- [x] Integration tests for end-to-end inference via OpenRouter

#### Groq Llama 3 Integration (`feat/groq-llama3`) ✅ **COMPLETED**

- [x] Install `groq` Python client
- [x] Initialise client using `GROQ_API_KEY` env variable
- [x] Support `llama3-70b-8192` (and smaller) models
- [x] Add provider adapter compatible with `LLMProvider` router
- [x] Implement retry & timeout logic matching other providers
- [x] Unit tests for Groq completion & error paths
- [x] Integration tests comparing latency vs OpenRouter

#### Groq Provider Hardening (`feat/groq-provider-hardening`) ✅ **COMPLETED**

#### Provider Robustness & Dependency Pinning (`fix/provider-robustness`)

- [x] Map Gemini aliases to OpenRouter canonical IDs (`gemini-2.0-flash` → `google/gemini-2.0-flash-001`)
- [ ] OpenRouter message format: if validation occurs, fall back to prompt-string mode (join messages)
- [x] Google provider: restrict kwargs to supported set (no unsupported `temperature` path)
- [ ] Groq: use valid `GROQ_API_KEY` or disable provider; rewrite quota-sharing to use `redis.asyncio` and `await` every call
- [ ] CI smoke: provider validation job hits `/api/v1/providers/validate` for enabled providers

- Standardized 8192-token context window for Groq models.
- Improved error handling, logging, and metrics integration.
- Added smoke tests covering Groq rate-limit and timeout scenarios.

#### Model Routing (`feat/model-routing`) ✅

- [x] Add optional `model` to `ChatRequest` (e.g., `gemini-2.0-flash`, `llama3-70b-8192`, `gpt-4o`)
- [x] Reorder providers dynamically per-request based on model alias:
  - `gemini*`/`google*` → prioritize Google/OpenRouter
  - `llama3*` → prioritize Groq
  - `gpt-*`/`chatgpt*` → prioritize OpenAI
- [x] Thread `model` through `LLMRouter.acomplete/astream_complete`
- [x] Tests: verify ordering for Gemini/Llama3/GPT aliases

#### LLMGateway Self-Hosted (`infra/llmgateway`) – *Planned*

- [ ] Fork `llmgateway/llmgateway` repo; pin Docker image tag in `docker-compose.yml`
- [ ] Create **Dockerfile** + Railway/DO App Spec for one-click deploy (8080)
- [ ] Add `LLMGATEWAY_BASE_URL` + `LLMGATEWAY_API_KEY` to `settings.py`
- [ ] Implement `LLMGatewayProvider` adapter compatible with `LLMProvider` interface
- [ ] Update `llm_router.py` priorities: LLMGateway → Groq → OpenAI → Local
- [ ] Migrate existing OpenRouter env vars to LLMGateway equivalents
- [ ] Unit tests: routing to gateway, cost reporting, error fallback
- [ ] Integration tests: end-to-end inference via self-hosted gateway
- [ ] Docs: self-hosting guide, cost model comparison, migration steps
- [ ] Provide ops guide with **two deployment options**:
      1. *Unified image* – single container (`ghcr.io/theopenco/llmgateway-unified:<tag>`) exposing ports 3002/3005/4001/4002
         ```bash
         docker run -d --name llmgateway --restart unless-stopped \
           -p 3002:3002 -p 3005:3005 -p 4001:4001 -p 4002:4002 \
           -v ~/llmgateway_data:/var/lib/postgresql/data \
           -e OPENAI_API_KEY=$OPENAI_API_KEY \
           -e AUTH_SECRET=$AUTH_SECRET \
           ghcr.io/theopenco/llmgateway-unified:${TAG}
         ```
      2. *Split services* – `docker-compose.split.yml` for separate API/UI/Gateway/DB/Redis containers
         ```bash
         git clone https://github.com/theopenco/llmgateway.git && cd llmgateway
         cp .env.example .env  # edit API keys
         docker compose -f infra/docker-compose.split.yml up -d
         ```
- [ ] Document **required env vars** (`POSTGRES_PASSWORD`, `AUTH_SECRET`, `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, …)
- [ ] Add make targets: `make llmgateway-up`, `make llmgateway-logs`, `make llmgateway-down`

### 3 · Schema & Docs Clean‑up (3C) - Detailed Tasks

#### Code Cleanup (`chore/drop-unused-models`)

- [ ] Remove `QueryLog` + stub `Vector` models
- [ ] Plan for real audit log implementation later
- [ ] Clean up dead code and unused imports

#### Incremental Ingestion (`feat/upsert-ingest`)

- [ ] Implement SHA-256 per page in ingestion script
- [ ] Add UPSERT logic for only new/changed chunks
- [ ] Add integration tests: update 1 page, DB rows count remains same

#### Documentation Sync (`chore/docs-sync`)

- [ ] Align environment variable names across docs
- [ ] Add doc-lint (markdown-link-check, doctoc) to pre-commit
- [ ] Add CI doc-lint job
- [ ] Update .env examples and README files

#### User & Developer Guides (`docs/user-dev-guides`)

- [ ] Author comprehensive API usage guide with request/response samples
- [ ] Document model selection & configuration options
- [ ] Provide Streamlit demo setup instructions for local & cloud
- [ ] Write deployment walkthrough (Docker, DigitalOcean/AWS)
- [ ] Add troubleshooting & FAQ section
- [ ] Include doc-lint coverage in CI

### 4 · Container Hardening (3D) - Detailed Tasks

#### Multi-stage Dockerfile (`infra/multistage-docker`)

- [ ] Implement builder → runtime stage separation
- [ ] Exclude dev dependencies from runtime
- [ ] Add non-root `app` user
- [ ] Add HEALTHCHECK endpoint
- [ ] CI job: ensure image < 400MB

#### Monitoring Integration (`infra/prometheus`) ✅ **COMPLETED**

- [x] Added `prometheus_fastapi_instrumentator`
- [x] Exposed `/metrics` endpoint
- [x] Added integration tests for scraping `http_requests_total`
- Consolidated under **Flexible Monitoring Architecture** section.

#### Deployment Pipeline (`infra/deploy-public`)

- [ ] Containerise combined FastAPI & Streamlit services
- [ ] Configure GitHub Actions CI/CD on push to `main` branch
- [ ] Deploy automatically to **DigitalOcean** (App Platform / Droplet)  
- [ ] Run smoke tests post-deploy & report status
- [ ] Perform load testing and document scaling strategy

#### Railway Deployment (`infra/railway-deploy`)

- [ ] Add `railway.json` project config with service & database definitions
- [ ] Provision managed **PostgreSQL** and **Redis** on Railway
- [ ] Build & deploy FastAPI Docker image via Railway pipeline
- [ ] Configure environment variables & secrets in Railway UI
- [ ] Enable per-PR preview environments for smoke testing
- [ ] GitHub Actions job: auto-deploy `main` branch to Railway
- [ ] Document Railway workflow & rollback strategy

#### Metabase BI Dashboards (`infra/metabase-analytics`)

- [ ] Deploy **Metabase** container on Railway (one-click template)
- [ ] Connect to production Postgres for read-only analytics
- [ ] Create dashboards for KPIs: vector latency, recall, LLM errors
- [ ] Embed Metabase dashboards in internal wiki / Streamlit demo
- [ ] Configure email/Slack alerts for SLA breaches
- [ ] Add health-check endpoint test in CI

### 5 · Testing Expansion (4A) - Detailed Tasks

#### Unit Testing Expansion (`test/unit-index`)

- [ ] Implement PGVector in-memory mock (sqlite+vector)
- [ ] Add comprehensive unit test coverage
- [ ] Target: 80% line coverage, 90% function coverage on `app/`

#### Integration Testing (`test/integration-rag`)

- [ ] Set up Postgres/Redis via pytest-docker
- [ ] Implement end-to-end RAG pipeline tests
- [ ] Test ingestion → query → response flow

#### E2E Testing (`test/e2e-stream`)

- [ ] Test `/chat` endpoint with long questions
- [ ] Assert proper streaming behavior
- [ ] Validate full system integration

#### Security Testing (`test/fuzz-heuristic`)

- [ ] Implement random prompt injection string testing
- [ ] Ensure proper sanitization
- [ ] Test malformed PDF handling
- [ ] Add heuristic fuzz testing

#### Coverage Gates

- [ ] Implement `pytest --cov=app --cov-fail-under=80`
- [ ] Add coverage reporting to CI/CD
- [ ] Set up coverage gate in branch protection

### 6 · Memory & Doc Management (4B) - Detailed Tasks

#### Conversation Memory (`feat/redis-memory`)

- [ ] Store **last 20-30 Q&A pairs** per `session_id` in Redis for context window
- [ ] **Persist memory across browser sessions** with optional user reset button
- [ ] Implement **30-day retention policy** with automatic soft deletion of old conversations
- [ ] Prepend conversation context to prompts
- [ ] Add unit tests for context length logic
- [ ] Implement memory cleanup strategies

#### Document Management API (`feat/doc-upload-api`)

- [ ] Implement `/api/v1/docs` POST for new file uploads
- [ ] Add PUT endpoint for document versioning
- [ ] **Implement soft deletion only** - never remove documents from database
- [ ] **Client-side validation for file size limit (10MB max)**
- [ ] Add integration tests: upload → ingest → query returns new content
- [ ] Implement file validation and security

#### Metadata Schema (`feat/metadata-schema`)

- [ ] Add `documents` table (id, path, version, uploaded_at, author, **file_size, page_count, content_hash**)
- [ ] Support **free-text tagging** system with tag management
- [ ] **Keep all document versions** with soft delete flags
- [ ] Implement metadata CRUD operations
- [ ] Add integration tests for metadata management
- [ ] Support document tagging and categorization

#### Rate Limit Status API (`feat/rate-limit-status`)

- [ ] Implement `/api/v1/rate-limit/status` endpoint returning remaining requests & reset time
- [ ] Include current window usage and next reset timestamp
- [ ] Add caching to avoid Redis overhead on frequent status checks
- [ ] Unit tests for status calculation accuracy
- [ ] Integration tests for status endpoint

#### Streamlit Demo (`feat/demo-ui`)

- [ ] Create Streamlit-based demo interface (`streamlit-chat` / `hugchat`)
- [ ] Support file uploads **PDF only for v1** with **separate "Process" button workflow**
- [ ] **Client-side validation: reject PDFs >10MB** with clear error message
- [ ] Allow **users to input their own API keys** (stored in session state, cleared on exit)
- [ ] Implement **API key sanitization middleware** to redact keys from logs
- [ ] Implement chat interface using `/api/v1/chat` with conversation history display
- [ ] Add model selection dropdown with default options:
      - "Gemini (OpenRouter)"
      - "Llama 3 (Groq)"
      - "ChatGPT (OpenAI)"
      - "Local (GPT4All)"
- [ ] **Expose model parameters**: temperature, max_tokens sliders in sidebar
- [ ] Enable streaming responses for answers > 300 characters
- [ ] Display document source & tag metadata alongside responses
- [ ] **Show rate limit info in sidebar** via `/api/v1/rate-limit/status` calls
- [ ] **Sidebar rate limit warnings** when approaching limits
- [ ] Unit tests for each UI feature
- [ ] End-to-end tests for upload + chat interaction
- [ ] Rate-limit message UX improvements

#### URL Scraping API (`feat/doc-scrape-api`)

- [ ] Implement `/api/v1/docs/scrape` accepting `{ urls: string[], respect_robots: bool }`
- [ ] Enforce **domain whitelisting** via `ALLOWED_SCRAPE_DOMAINS` (default deny)
- [ ] Fetch with timeouts, retries, and robots.txt compliance (toggle)
- [ ] Parse & sanitize HTML → text; chunk with `SentenceSplitter`
- [ ] Index via `PGVectorStore` (respect `EMBEDDING_DIM`); trace steps in **Langfuse**
- [ ] Integration tests: scrape → ingest → query returns new content

#### Vector DB Explorer (`feat/vector-explorer`)

- [ ] Implement `/api/v1/vector/stats` (row_count, distinct sources, embedding dim)
- [ ] Implement `/api/v1/vector/rows` with filters: `source_document`, `limit`, `offset`, and `q` for semantic search, `top_k` (configurable)
- [ ] Return **raw ANN scores** and configurable **content preview length**
- [ ] Protect endpoints with **admin auth** (beyond API key)
- [ ] Add SQLAlchemy queries for pagination + LlamaIndex-backed similarity search when `q` provided
- [ ] Integration tests for stats and row listing; verify performance under pagination

#### Chat History Persistence & API (`feat/chat-history`)

- [ ] Create `chat_logs` table (id, session_id, provider_name, model_name, query_text, response_text, created_at)
- [ ] Persist chat logs in `/api/v1/chat` handler including **which LLM provider/model** served the response
- [ ] Implement `/api/v1/chats` with filters: `session_id`, date range, `limit`, `offset`; protect with **admin auth** for global listing
- [ ] Add export (CSV/JSON) support at API level
- [ ] Retain full history (no deletion); document retention policy and storage growth considerations
- [ ] Integration tests: chat → persisted → retrieval API returns rows

#### Streamlit Multi-Page UI (`feat/demo-ui-pages`)

- [ ] Convert demo to multi-page app:
  - [ ] Chat (existing `ui/demo_ui.py`)
  - [ ] 1_Ingest_PDFs: upload PDFs → call `/api/v1/docs/upload` with size/type validation
  - [ ] 2_Scrape_Websites: paste URLs → call `/api/v1/docs/scrape`; show per-URL status
  - [ ] 3_Vector_DB_Explorer: call `/api/v1/vector/stats` and `/api/v1/vector/rows`; filters + semantic search
  - [ ] 4_Chat_History: call `/api/v1/chats`; filter by `session_id`, export CSV
- [ ] Reuse API error-handling patterns from current Chat page
- [ ] Add progress indicators and user-friendly toasts/status updates
- [ ] Enforce **10MB per PDF** client-side and server-side
- [ ] Require **admin auth** for Vector Explorer and Chat History pages
- [ ] Ship Streamlit and FastAPI in the **same container** for now

#### Embedding Dimension Alignment (`chore/embed-dim-sync`)

- [ ] Ensure `settings.EMBEDDING_DIM` matches DB column type and migrations (target **1536** for Gemini default)
- [ ] Verify ingestion scripts and query engine use the same dimensionality
- [ ] Add a linter/test that fails if configured dim ≠ DB dim

#### Reconcile Query Logging Plan (`chore/chat-log-schema`)

- [ ] If `QueryLog` is removed per `chore/drop-unused-models`, introduce `chat_logs` replacement with a migration
- [ ] If retained, standardize it for chat history use (indexes, created_at, provider fields) and update docs accordingly
- [ ] Document retention policy (e.g., 30 days) and PII handling

#### Incremental Ingestion & Versioning (`feat/upsert-ingest` + `feat/metadata-schema`)

- [ ] Compute **page-level SHA-256** hashes during ingestion; **UPSERT only changes**
- [ ] Enforce **10MB PDF limit**; validate MIME and extension
- [ ] Add `documents` metadata table with fields: `id, path, version, uploaded_at, author, tags, file_size, page_count, content_hash`
- [ ] Prevent duplicate ingestion by checking `content_hash` and `version`; maintain full version history
- [ ] Integration tests: editing 1 page keeps row count stable; new version creates new metadata entry

#### Document Storage Backend (`feat/doc-storage`)

- [ ] Initial storage to local filesystem under `uploaded_docs/{datetime}/{filename}_{version}.pdf`
- [ ] Config flags: `STORAGE_BACKEND=local|minio`, `UPLOADED_DOCS_DIR=uploaded_docs/`
- [ ] Persist `storage_backend` and `storage_uri` in `documents` table (e.g., `file:///...` or `s3://bucket/key`)
- [ ] Ensure safe filenames, auto-create date-based directories, and 10MB server-side validation
- [ ] E2E: upload → file written at expected path → metadata row created with correct URI

#### MinIO Storage Migration (`feat/minio-migration`)

- [ ] Add MinIO integration (S3-compatible): `MINIO_ENDPOINT`, `MINIO_ACCESS_KEY`, `MINIO_SECRET_KEY`, `MINIO_BUCKET`, `MINIO_SECURE`
- [ ] Implement write/read client and optional pre-signed URL generation for downloads
- [ ] Migration job: move existing local files to MinIO; update `storage_backend` and `storage_uri`
- [ ] Switch `STORAGE_BACKEND=minio` via environment without code changes
- [ ] Integration tests for MinIO pathing and metadata updates

#### Embedding Migration for Existing Data (`feat/embedding-migration`)

- [ ] Implement a re-embedding job to recompute vectors using the selected provider/dimension (Gemini 1536 by default)
- [ ] Provide safe **in-place update** of `content_vector` or dual-write strategy during migration
- [ ] Add CLI and background task to migrate existing rows; include progress reporting
- [ ] Backfill tests: after migration, search quality and latency remain within targets

#### Admin Authentication for Sensitive APIs (`feat/admin-auth`)

- [ ] Require authentication beyond API key for admin endpoints: `/vector/*`, `/chats` (global)
- [ ] Introduce `ADMIN_TOKEN` or RBAC (upgrade to OAuth 2.1 PKCE per 4C later); gate Streamlit admin pages
- [ ] Integration tests validating unauthorized access is blocked

#### Langfuse Tracing Coverage (`feat/langfuse-ingest-rag`)

- [ ] Add Langfuse spans for ingestion and scraping steps (load → parse → chunk → embed → store)
- [ ] Ensure `/api/v1/chat` continues to trace the **RAG pipeline** with correlated `trace_id`
- [ ] Document masking/redaction for inputs/outputs and metadata

### 7 · Strategic Performance & Security Upgrades (4C) - High Impact Tasks

#### Vector Index Performance Optimization (`feat/hnsw-migration`) ✅ **COMPLETED** — see "Database Optimization" section for full metrics and benchmarks

#### Streaming Performance Enhancement (`feat/streaming-optimization`)

- [ ] **Adopt Server-Timing + SSE** to mask backend latency
- [ ] Test with 100KB token responses for streaming stability
- [ ] Implement chunked response optimization for FastAPI
- [ ] Add streaming latency monitoring and alerting
- [ ] Benchmark: streaming vs non-streaming performance comparison

#### Next-Gen Embedding Integration (`feat/gemini-embeddings`) ⚑ **SPECULATIVE**

- [ ] **Embed Gemini 2.5 Pro** for multilingual RAG capabilities
- [ ] Evaluate via Google AI Studio SDK integration
- [ ] Add embedding model A/B testing framework
- [ ] Implement embedding model fallback strategies
- [ ] Performance comparison: Gemini vs OpenAI vs HuggingFace embeddings

#### Security & Compliance Hardening (`feat/security-hardening`) 🔒 **CRITICAL**

- [ ] **OAuth 2.1 PKCE implementation** for secure authentication
- [ ] TLS mTLS option for service-to-service communication
- [ ] Secrets management integration (AWS/GCP Secret Manager) ⚑
- [ ] Rate limiting per authenticated user (beyond IP-based)
- [ ] Security audit logging and monitoring

**Acceptance Criteria:**

- Pass OWASP security scanning with zero high/critical vulnerabilities
- Implement OAuth 2.1 with PKCE flow for secure authentication
- All secrets stored in external secret management (no hardcoded values)
- Comprehensive audit trail for all authenticated operations
- User-based rate limiting prevents abuse by individual accounts

#### Supply Chain Security (`feat/sbom-security`)

- [ ] **Add SBOM & SLSA-3 GitHub workflow** for vulnerability tracking
- [ ] Pin dependency hashes with `uv pip compile --generate-hashes`
- [ ] Implement automated dependency vulnerability scanning
- [ ] Add license compliance checking
- [ ] Container image scanning beyond Trivy (Syft/Grype integration)

#### Advanced Observability (`feat/opentelemetry`)

- [ ] **Enable OpenTelemetry autoinstrumentation** for comprehensive tracing
- [ ] Export to Tempo/Grafana for trace-log metrics fusion
- [ ] Implement distributed tracing across LLM providers
- [ ] Add custom metrics for RAG pipeline performance
- [ ] Correlation between Langfuse traces and OpenTelemetry spans

#### Property-Based Testing Enhancement (`feat/property-testing`)

- [ ] **Property-based tests** using `hypothesis` for schema edge-cases
- [ ] Extend fuzz testing to cover more attack vectors
- [ ] Add mutation testing to validate test suite quality
- [ ] Implement chaos engineering tests for provider failures
- [ ] Performance regression testing in CI/CD

#### Production Deployment Automation (`feat/production-deployment`)

- [ ] **Release smoke-test container** with 1-click DigitalOcean App Spec
- [ ] Pre-seed embeddings to showcase instant query capability
- [ ] Automated health checks and rollback mechanisms
- [ ] Blue-green deployment strategy implementation
- [ ] Production monitoring dashboard and alerting

### 8 · Optional Features (4D) - Deferred Tasks

#### User Feedback System

- [ ] Implement feedback collection API
- [ ] Integrate with Langfuse for quality tracking
- [ ] Add feedback-based model improvements

#### WordPress Plugin

- [ ] Develop WordPress integration
- [ ] Create plugin for easy embedding
- [ ] Document installation and configuration

#### Serverless PoC

- [ ] Explore serverless deployment options
- [ ] Implement cost optimization strategies
- [ ] Create deployment automation

#### Langfuse Deep Tracing (`feat/langfuse-trace`)

- [ ] Add `langfuse` SDK dependency and initialise client via settings (no hard-coded secrets)
- [ ] Decorate LLM & RAG calls with `@trace` to capture spans
- [ ] Implement **custom masking function** to redact sensitive data before transmission
- [ ] Ensure sampling configuration & redaction rules cover inputs/outputs & metadata
- [ ] Integration tests verifying traces appear in Langfuse dashboard
- [ ] Update docs with observability guidelines

#### DOCX & URL Ingestion (`feat/docx-url-ingest`)

- [ ] Extend ingestion pipeline to accept DOCX and remote URLs
- [ ] Update `/api/v1/docs` validation & chunking logic for new formats
- [ ] Update metadata schema to store original URL & file type
- [ ] Integration tests: ingest DOCX & URL then query returns content
- [ ] Update docs & demo once feature is stable

## Testing Methodologies

**Unit** – stateless pure functions; LLMs mocked

**Integration** – local Postgres / Redis via pytest‑docker; run ingest → query

**E2E** – docker‑compose full stack; HTTP smoke tests

**Heuristic / Fuzz** – adversarial prompts, malformed PDFs

**Coverage Gate** – pytest --cov=app --cov-fail-under=80

## Runtime Baseline

- Python: 3.11 (project-wide). Tooling (ruff target-version), tests, and container use Python 3.11.

## Branch Protection & CI Matrix

| Job | Trigger | Steps |
|-----|---------|-------|
| lint | PR | ruff, pyupgrade, mypy |
| unit-test | PR | pytest -m unit |
| integration-test | PR label integration or main | spin DB / Redis, run tests |
| docker-build | tag on main | build, Trivy scan, push ghcr.io |
| doc-lint | PR | markdownlint‑cli2, link‑check |

## Git Branching & PR Workflow

```
main             # always deployable
└── develop      # integration
    ├── feat/<slug>/…
    ├── fix/<slug>/…
    ├── chore/<infra‑slug>/…
    └── test/<slug>/…
```

**Rule per PR**: One topic, ≤ 400 LoC, checklist must pass.

## Release Tags

**v0.x.y** # dev/demo
**v1.x.y** # first prod cut

Tag triggers Docker build → DigitalOcean Container Registry deploy.

## Implementation Guidance (High Priority Tasks)

### HNSW Migration Steps (`feat/hnsw-migration`)

```bash
# 1. Check pgvector version (ensure 0.5+)
docker exec chatbot-api-service-postgres-1 psql -U postgres -c "SELECT * FROM pg_extension WHERE extname = 'vector';"

# 2. Create migration script
alembic revision --autogenerate -m "migrate_to_hnsw_index"

# 3. Performance benchmark before/after
python scripts/benchmark_vector_search.py --index-type ivfflat
python scripts/benchmark_vector_search.py --index-type hnsw

# 4. Monitor query performance
curl http://localhost:8000/metrics | grep vector_search_duration
```

### Redis Cache Implementation (`feat/redis-cache`)

```bash
# 1. Test cache coherence across workers
docker-compose exec app python -c "
from app.core.cache import get_redis_cache
cache = get_redis_cache()
cache.set('test_key', 'worker_1_value', ttl=60)
print('Cache set:', cache.get('test_key'))
"

# 2. Cache performance testing
ab -n 1000 -c 10 http://localhost:8000/api/v1/chat \
   -H "Content-Type: application/json" \
   -H "X-API-Key: test" \
   -p cached_query.json
```

### Security Implementation (`feat/security-hardening`)

```bash
# 1. Generate OAuth keys
openssl rand -hex 32  # OAuth client secret
openssl rand -hex 16  # PKCE code verifier

# 2. Test OAuth flow
curl -X POST http://localhost:8000/auth/token \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "grant_type=authorization_code&code=AUTH_CODE&client_id=CLIENT_ID"

# 3. SBOM generation
uv pip install cyclonedx-bom
cyclonedx-py -o sbom.json
```

## Development Commands (Phase 4) - 17 Total Commands

```bash
# System Management
make up                     # start services
make down                  # stop services  
make logs                  # view logs
make health-check          # comprehensive system health verification

# Development & Testing
make test                  # all tests (95 pass)
make test-cov             # tests with coverage
make test-pgvector        # pgvector performance tests
make test-metrics         # flexible metrics system tests
make benchmark            # vector search performance benchmarks
make lint                 # code quality  
make format               # format code
make quality-check        # all quality checks (lint + format + type)
make shell                # container shell
make chat                 # test chat endpoint

# Database Operations
make db-shell             # psql
make db-status            # database & pgvector status
make ingest               # re‑ingest PDFs

# Maintenance & Dependencies
make clean                # cleanup containers/volumes
make deps-install         # install/sync dependencies
make deps-update          # update dependencies
make deps-compile         # compile requirements

# Performance Verification
# Baseline eval
docker exec chatbot-api-service-app-1 python scripts/run_baseline_evaluation.py
# Smoke test
curl -X POST "http://localhost:8000/api/v1/chat" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer <token>" \
     -d '{"question":"test","session_id":"test"}'
```

**✅ pgvector Performance Achieved:**

- **P99 latency: 1.88ms** (target <50ms) — 27× better than target
- **Recall: 100%** at top-1, top-5, top-10 (target ≥95%)
- **HNSW index**: m=32, ef_construction=64, ef_search=100

## 🔧 **Flexible Monitoring Architecture** ✅

**Design Philosophy**: Support multiple monitoring backends through a pluggable architecture.

### **Supported Monitoring Tools**

| Backend | Configuration | Use Case |
|---------|---------------|----------|
| **Prometheus** ✅ | `METRICS_BACKEND=prometheus` | Default for Kubernetes/Docker deployments |
| **DataDog** ✅ | `METRICS_BACKEND=datadog` + `DATADOG_API_KEY` | SaaS APM, enterprise environments |
| **OpenTelemetry** ✅ | `METRICS_BACKEND=opentelemetry` | Cloud-native, vendor-neutral observability |
| **NoOp** ✅ | `METRICS_BACKEND=noop` | Development/testing (no metrics overhead) |
| **Auto-detection** ✅ | `METRICS_BACKEND=auto` (default) | Automatically detects available libraries |

### **Available Metrics**

| Metric | Type | Description | Labels |
|--------|------|-------------|---------|
| `vector_search_duration_seconds` | Histogram | Vector similarity search latency | `status`, `model` |
| `vector_search_requests_total` | Counter | Total vector search requests | `status`, `model` |
| `vector_search_recall` | Gauge | Search recall accuracy | `k` (top-k results) |

### **Configuration Examples**

```bash
# Prometheus (default if prometheus_client installed)
METRICS_BACKEND=prometheus
PROMETHEUS_ENABLED=true

# DataDog (requires datadog library + API key)
METRICS_BACKEND=datadog
DATADOG_API_KEY=your-api-key-here

# OpenTelemetry (cloud-native, vendor-neutral)
METRICS_BACKEND=opentelemetry

# No metrics (development/testing)
METRICS_BACKEND=noop
```

### **Easy Migration Between Monitoring Tools**

The flexible architecture allows switching monitoring backends with **zero code changes**:

1. **Development** → **Production**: `noop` → `prometheus`
2. **Self-hosted** → **SaaS**: `prometheus` → `datadog`
3. **Vendor-neutral**: Any backend → `opentelemetry`
4. **Cloud migration**: `prometheus` → cloud provider's native metrics

✅ **Already implemented!** The current architecture supports:

- **New Relic**: Add `NewRelicBackend` class implementing the `MetricsBackend` interface
- **Grafana Cloud**: Works with Prometheus backend + remote write
- **AWS CloudWatch**: Add `CloudWatchBackend` with boto3 integration
- **Custom solutions**: Implement `MetricsBackend` interface for any tool

**Next steps if switching:**

1. Set `METRICS_BACKEND=your_preferred_tool` in environment
2. Add the monitoring library to `requirements.in`
3. If needed, implement a new backend class (following the existing pattern)

**Zero migration effort** — just change the environment variable!

## Quality Metrics (Achieved)

✅ **128 tests** ✅ **Chat coverage** ✅ **Lint & type clean** ✅ **Secrets scan & API‑key auth** ✅ **Docs & guides** ✅ **Langfuse tracing** ✅ **Structured logging** (JSON format, correlation IDs, sensitive data masking) ✅ **LLM Router production stability** (logger fixes, error handling, 100% test pass rate) ✅ **pgvector HNSW migration** (P99: 1.88ms, 100% recall, 27× faster than target) ✅ **Redis LRU Cache** (400MB allkeys-lru policy, cache coherence, ≥80% hit rate, 33 tests) ✅ **Flexible monitoring architecture** (4 backend support: Prometheus/DataDog/OpenTelemetry/NoOp) ✅ **Enhanced Makefile** (17 commands for comprehensive development workflow)

## Technical Assessment & Strategic Roadmap

**TL;DR** Solid modern FastAPI + RAG stack: structured logs, Redis rate-limits, multi-stage Docker, **HNSW vector index**, **Redis LRU cache**—excellent DevEx. ✅ **Completed**: pgvector HNSW migration (27× faster than target), Redis LRU cache (eliminates split-brain). Remaining gaps: streaming latency, RBAC/OAuth, and advanced observability.

### ✅ Strong Points

| Area | Why it matters |
|------|----------------|
| **Structured JSON logging + correlation-IDs** | Aligns with proven structlog patterns, simplifies ELK/Grafana search |
| **Redis fixed-window rate-limit** | Follows recommended FastAPI recipe, avoids DOS spikes |
| **Multi-stage Dockerfile (<400 MB)** | Cuts 200 MB+ by stripping build deps; PythonSpeed pattern |
| **HNSW vector index + pgvector** ✅ | **P99: 1.88ms latency** (27× faster than target), 100% recall accuracy |
| **Redis LRU Cache + coherence** ✅ | **400MB allkeys-lru policy**, eliminates split-brain across workers, ≥80% hit rate |
| **Flexible metrics framework** ✅ | **Monitoring-agnostic design** supports Prometheus, DataDog, New Relic, OpenTelemetry |
| **LLM router + provider fallback** | Mitigates upstream outages; mirrors multi-provider best practice |
| **81+ test gate & CI matrix** ✅ | Strong quality bar, performance benchmarks, fuzz hooks show security awareness |

### ⚠️ Remaining Weak Points

| Gap | Impact | Reference |
|-----|--------|-----------|
| **StreamingResponse** | Known FastAPI throughput drop vs chunked Flask; risk for >300 char answers | [FastAPI streaming issues](https://github.com/fastapi/fastapi/issues/2302) |

| **Embedding parity test ±0.02** | HF & OpenAI/Gemini vectors diverge more on domain-specific texts; false failures likely | [embedding comparison](https://softwaremill.com/embedding-models-comparison/) |
| **Auth / RBAC absent** | No OAuth/JWT = PHI leak potential | Security best practices |
| **SBOM / supply-chain scan** | Trivy only on image; pip deps unpinned transitive vulnerabilities remain | SLSA framework compliance |
| **Observability** | Langfuse traces good, but no OpenTelemetry spans or log–trace correlation | OpenTelemetry integration |

## Immediate Priorities (Top 4)

1. ~~**HNSW Vector Index Migration**~~ ✅ **COMPLETED** (27× performance improvement)
2. ~~**Redis LRU Cache**~~ ✅ **COMPLETED** (400MB allkeys-lru, cache coherence, split-brain eliminated)
3. **Streaming Responses** – implement `feat/streaming` (chunked StreamingResponse >300 chars).
4. **Security Hardening** – branch `feat/oauth-rbac`, add OAuth 2.1 PKCE authentication.

### Next Critical Step (to implement now)

- Branch: `feat/oauth-rbac`
  - Implement OAuth 2.1 PKCE, protect admin endpoints, and add user-based rate limiting integration
  - Tests: auth flow unit tests, integration protection on `/vector/*`, `/chats`, and UI gating

✅ **COMPLETED**: Redis Rate-Limit, Structured Logging, LLM Router, LLM Router Production Fixes, **Redis LRU Cache**

✅ **PRODUCTION READY**: Full LLM Router implementation with 70 passing tests, comprehensive error handling, structured logging, and automatic provider fallback.

Complete in sequence; each merged PR closes its table entry.

## Package Management with uv

When adding new packages to the project:

1. **Add the package to the appropriate requirements file**:
   - For production dependencies: Add to `requirements.in`
   - For development dependencies: Add to `requirements-dev.in`

2. **Compile the requirements**:

   ```bash
   # For production dependencies
   uv pip compile requirements.in -o requirements.txt
   
   # For development dependencies
   uv pip compile requirements-dev.in -o requirements-dev.txt -c requirements.txt
   ```

3. **To upgrade all packages to their latest versions**:

   ```bash
   # For production dependencies
   uv pip compile --upgrade requirements.in -o requirements.txt
   
   # For development dependencies
   uv pip compile --upgrade requirements-dev.in -o requirements-dev.txt -c requirements.txt
   ```

4. **To install the exact versions specified**:

   ```bash
   # Install both production and development dependencies
   uv pip sync requirements.txt requirements-dev.txt
   
   # For production only
   uv pip sync requirements.txt
   ```

Always run these commands after adding new dependencies to ensure consistent environments across development and production.
