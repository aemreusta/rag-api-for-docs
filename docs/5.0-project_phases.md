# Project Phases

This updated plan leverages the new frameworks to accelerate development and focus on quality.

## Phase 1: Setup & Data Ingestion ‚úÖ **COMPLETED**

**Goal**: Establish the core infrastructure and create a high-quality, indexed knowledge base.

**Details**:

- ‚úÖ **Setup**: Configure the environment with FastAPI, LlamaIndex, Langfuse, Docker, Postgres, and Redis.
- ‚úÖ **Ingestion Script**: Refactor `scripts/ingest.py` to use LlamaIndex's PyMuPDFReader and SentenceSplitter (or a more advanced node parser).
- ‚úÖ **Indexing**: Configure the PGVectorStore in LlamaIndex and run the ingestion script to populate the database with embeddings.
- ‚úÖ **Observability**: Integrate the Langfuse SDK from day one to trace the ingestion process.
- ‚úÖ **Quality Assurance**: Comprehensive testing (7/7 tests passing), pre-commit hooks, and code quality validation.

**Outcome**: ‚úÖ **ACHIEVED** - A versioned, indexed knowledge base in PostgreSQL and a repeatable ingestion script.

**Current Status**:

- API server running on <http://localhost:8000>
- PostgreSQL with pgvector extension enabled
- Redis cache operational
- Langfuse observability platform running on <http://localhost:3000>
- PDF documents successfully indexed (sample_policy.pdf)
- All tests passing (35% code coverage)
- Pre-commit hooks and quality gates implemented

## Phase 2: Query Engine Development & Evaluation üöß **IN PROGRESS**

**Goal**: Build and validate the core RAG pipeline that can answer questions accurately.

### Todo List

- ‚úÖ **Build Core Query Engine**: Implement `core/query_engine.py` with:
  - ‚úÖ Initialize PGVectorStore with content_embeddings table
  - ‚úÖ Configure OpenRouter LLM integration
  - ‚úÖ Set up VectorStoreIndex
  - ‚úÖ Implement CondenseQuestionChatEngine for conversational context
  - ‚úÖ Expose get_chat_response function
- üîÑ **API Endpoint**: Wire the query engine to the FastAPI endpoint in `chat.py`.
- ‚è≥ **Create Dataset**: In Langfuse, create a "golden dataset" of 20-50 important questions with ideal answers.
- ‚è≥ **Evaluate**: Run your first evaluations using Langfuse to score the baseline engine on metrics like Faithfulness, Answer Relevancy, and Context Precision.

**Outcome**: A functional API endpoint whose quality is tracked and measured.

**Next Steps**:

1. Implement chat endpoint in `app/api/v1/chat.py`
2. Wire query engine to FastAPI routes
3. Create evaluation dataset in Langfuse
4. Run initial quality assessments

## Phase 3: Advanced Features & Deployment ‚è≥ **PLANNED**

**Goal**: Implement conversational memory and deploy the secure, observable service.

**Details**:

- ‚úÖ **Conversational Memory**: Upgrade the QueryEngine to a ChatEngine using LlamaIndex's built-in memory components to handle follow-up questions.
- **Security & Rate Limiting**: Implement the API key authentication and Redis-based rate limiting. These remain outside the LlamaIndex logic.
- **Containerize & Deploy**: Finalize the Dockerfile and docker-compose.yml (including the Langfuse container if self-hosting) and deploy the stack to a cloud provider.
- **Integrate**: Work with the frontend team, providing the API spec and explaining how to capture user feedback scores to send to the Langfuse API.

**Outcome**: A smart, secure, and observable chatbot is live and accessible to the public.

## Phase 4: Continuous Improvement & Tuning ‚è≥ **PLANNED**

**Goal**: Use the observability platform to systematically improve the chatbot's performance, cost, and quality.

**Details**:

- **Monitor**: Actively monitor the Langfuse dashboards for cost spikes, latency degradation, or high rates of low-scoring user feedback.
- **Debug**: Use the detailed traces in Langfuse to analyze and fix any user-reported issues or bad responses.
- **A/B Test**: Use the Langfuse evaluation tools to A/B test different prompts, LLMs (via OpenRouter), or LlamaIndex retriever settings to find the optimal configuration.
- **Tune**: Based on data, make informed decisions to tune the system for better performance and user satisfaction.

**Outcome**: A mature, data-driven workflow for maintaining and enhancing a high-quality AI service.

## Development Commands for Current Phase

With Phase 1 complete and Phase 2 in progress, these are the key commands for ongoing development:

```bash
# System Management
make up          # Start all services
make down        # Stop all services
make logs        # View service logs

# Development
make test        # Run tests (should show 7/7 passing)
make lint        # Check code quality
make format      # Format code
make shell       # Access app container

# Database
make db-shell    # Access PostgreSQL
make ingest      # Re-run data ingestion
```

## Quality Metrics Achieved

- ‚úÖ **Tests**: 7/7 passing
- ‚úÖ **Coverage**: 35% (appropriate for infrastructure phase)
- ‚úÖ **Linting**: All checks pass
- ‚úÖ **Security**: Secrets scanning enabled
- ‚úÖ **Documentation**: Comprehensive setup guides
- ‚úÖ **Observability**: Langfuse integration ready
