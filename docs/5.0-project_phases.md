# Project Phases

This updated plan merges the original roadmap with the latest revisions to keep every task current and traceable.

## Phase 1 – Setup & Data Ingestion ✅ **COMPLETED**

**Goal**: Establish the core infrastructure and create a high‑quality, indexed knowledge base.

**Key deliverables** (all completed): FastAPI stack, LlamaIndex ingestion, PGVectorStore, Langfuse tracing, full test & lint gate.

## Phase 2 – Query Engine Development & Evaluation ✅ **COMPLETED**

**Goal**: Ship a measured baseline RAG pipeline with an authenticated /chat endpoint and evaluation dataset.

**Key deliverables** (all completed): VectorStoreIndex query engine, OpenRouter LLM, golden‑dataset, Langfuse evals, CI‑guarded quality metrics.

## Road Map (Phase 3 → Phase 4)

| Phase | Goal | Key Deliverables |
|-------|------|------------------|
| 3A – Robust Core | Harden monolith for low/medium traffic | ✅ sync API workers, in‑mem cache, PGVector column + GIN index, ✅ Redis rate‑limit, Langfuse try/catch, ✅ structured logs, custom error codes |
| 3B – LLM & RAG Upgrades | Model fallbacks, embeddings, streaming | ✅ provider router (Gemini / Groq / ChatGPT + local), embedding switch, streaming responses, default "sorry" fallback |
| 3C – DX & Docs | Code, schema, docs hygiene | ✅ dead‑code drop, checksum UPSERTs, doc‑lint CI |
| 3D – Prod‑Ready Container | Secure slim image | ✅ multi‑stage Dockerfile, non‑root user, HEALTHCHECK |
| 4A – Testing Maturity | ≥80 % critical‑path coverage | ✅ unit, integration, E2E, fuzz, coverage gate |
| 4B – Feature Growth | Memory, doc API, versioning | ✅ Redis convo memory, doc upload/version, metadata, Streamlit demo |
| 4C – Optional | Feedback, WordPress, serverless PoC | Defer until needed |

## 🔎 Detailed To‑Do Backlog (Authoritative)

### 1 · Core Hardening (3A)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/rate-limit-redis` | Fixed‑window per‑IP limit → 429 w/ reset‑after header | Unit window math; Integration exceed limit via pytest‑httpx |
| `feat/cache-lru` | cachetools.TTLCache on get_chat_response keyed by question | Unit hit/miss timing |
| `chore/cors-env` | Centralise settings.ALLOW_ORIGINS; default * in DEBUG | Unit env override |
| `feat/structured-logs` ✅ | JSON structured logging w/ correlation IDs, sensitive data masking, rotation | ✅ Unit tests (20), file creation & rotation |
| `fix/pgvector-type` | Migrate content_vector TEXT → VECTOR(1536) + USING ivfflat GIN | Integration ≤ 50 ms 1 k‑vec similarity |

### 2 · LLM & RAG Upgrades (3B)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/llm-router` | Priority‑based provider router w/ timeouts & automatic fallback | Unit forced timeout chains |
| `feat/embedding-switch` | Gemini / ChatGPT embeddings with fallback to HF | Integration cosine within ±0.02 |
| `feat/streaming` | StreamingResponse when answer > 300 chars | E2E client chunk reception |
| `feat/fallback-msg` | Generic apology + trace‑id on unhandled error | Unit MockLLMError |

### 3 · Schema & Docs Clean‑up (3C)

| Branch | Task | Tests |
|--------|------|-------|
| `chore/drop-unused-models` | Delete QueryLog, stub Vector | — |
| `feat/upsert-ingest` | SHA‑256 per page, UPSERT new/changed chunks | Integration rows stable on 1‑page edit |
| `chore/docs-sync` | Align env vars, add doc‑lint to pre‑commit | CI doc‑lint job |

### 4 · Container Hardening (3D)

| Branch | Task | Tests |
|--------|------|-------|
| `infra/multistage-docker` | Builder→runtime stages, strip dev deps, non‑root app, HEALTHCHECK | CI image < 400 MB |
| `infra/prometheus` | prometheus_fastapi_instrumentator, /metrics endpoint | Integration scrape returns http_requests_total |

### 5 · Testing Expansion (4A)

| Branch | Task | Scope |
|--------|------|-------|
| `test/unit-index` | PGVector in‑mem mock (sqlite+vector) | Unit |
| `test/integration-rag` | Spin Postgres/Redis via pytest‑docker | Integration |
| `test/e2e-stream` | Long‑Q streaming test on /chat | E2E |
| `test/fuzz-heuristic` | Random prompt‑injection & malformed PDF | Heuristic |

**Target coverage gate**: 80 % lines, 90 % functions on app/.

### 6 · Memory & Doc Management (4B)

| Branch | Task | Tests |
|--------|------|-------|
| `feat/redis-memory` | Persist last N Q&A by session_id; prepend to prompt | Unit context‑length enforcement |
| `feat/doc-upload-api` | /api/v1/docs POST/PUT for upload + version | Integration ingest then query new content |
| `feat/metadata-schema` | documents table (id, path, version, uploaded_at, author, tags) | Integration CRUD |

## 📝 Detailed Implementation Steps

### 1 · Core Hardening (3A) - Detailed Tasks

#### Rate Limiting Implementation (`feat/rate-limit-redis`) ✅ **COMPLETED**

- [x] Implement per-IP fixed window Redis-based rate limiting
- [x] Return 429 status with 24h reset message on limit exceeded
- [x] Add unit tests for window calculation edge cases
- [x] Add integration tests via pytest-httpx for rate limit scenarios

#### Caching & Performance (`feat/cache-lru`)

- [ ] Add `cachetools.TTLCache` decorator on `get_chat_response` (key=`question`)
- [ ] Implement cache hit/miss timing tests
- [ ] Configure appropriate TTL for chat responses

#### CORS Configuration (`chore/cors-env`)

- [ ] Move CORS origins to `settings.ALLOW_ORIGINS`
- [ ] Set default `*` in DEBUG mode
- [ ] Add unit tests for environment override

#### Structured Logging (`feat/structured-logs`) ✅ **COMPLETED**

- [x] Implement `logging.config.dictConfig` with structlog
- [x] Add JSON format logging with correlation IDs (request_id, trace_id)
- [x] Implement sensitive data masking (API keys, passwords, emails, Bearer tokens)
- [x] Add middleware for request/response logging with timing
- [x] Integrate correlation IDs with Langfuse traces
- [x] Add log file pattern `logs/app_%(process)d_%Y%m%d.log` with rotation (100MB)
- [x] Implement CompressingRotatingFileHandler with gzip compression
- [x] Add environment-based configuration (LOG_LEVEL, LOG_JSON, LOG_TO_FILE, etc.)
- [x] Add comprehensive unit tests for all logging components (20 tests)
- [x] Add rate limiting event logging with retry-after information
- [x] Update .gitignore and .dockerignore to exclude log files

#### Database Optimization (`fix/pgvector-type`)

- [ ] Replace `content_vector TEXT` → `VECTOR(1536)` via Alembic migration
- [ ] Add `USING ivfflat` GIN index for performance
- [ ] Add integration tests for similarity query latency < 50ms on 1k vectors

### 2 · LLM & RAG Upgrades (3B) - Detailed Tasks

#### LLM Router Implementation (`feat/llm-router`)

- [ ] Abstract `LLMProvider` with priorities and timeouts
- [ ] Implement **automatic fallback on any error** (timeout, rate limit, 5xx, auth failures)
- [ ] Set **30s default timeout** per provider (configurable via env vars later)
- [ ] **Cache failed providers temporarily** (5-minute cooldown) to avoid repeated failures in session
- [ ] **Route supported providers:** Gemini(OpenRouter) → Groq(Llama3) → ChatGPT → Local
- [ ] Add unit tests: forced timeout triggers next provider in chain
- [ ] Support streaming & non-streaming pathways

#### Embedding Provider Switch (`feat/embedding-switch`)

- [ ] Implement embedding via Gemini/ChatGPT with provider flag
- [ ] Add fallback to HuggingFace model
- [ ] Add integration tests for identical cosine score ±0.02

#### Streaming Responses (`feat/streaming`)

- [ ] Return `StreamingResponse` when answer > 300 characters
- [ ] Add E2E tests for client receiving chunks
- [ ] Implement proper error handling in streaming

#### Fallback Messaging (`feat/fallback-msg`)

- [ ] On unhandled exceptions, send 200 with predefined apology + trace ID
- [ ] Add unit tests with `MockLLMError`
- [ ] Implement graceful degradation patterns

#### OpenRouter Integration (`feat/openrouter`)

- [ ] Add `llama-index-llms-openrouter` dependency
- [ ] Initialize OpenRouter client with `OPENROUTER_API_KEY` env variable
- [ ] **Enable Google Gemini models via direct Google AI Studio API key integration**
- [ ] **Make default model configurable** via `DEFAULT_LLM_MODEL` env variable
- [ ] Expose model choice in settings & `/chat` endpoint (`model` query param)
- [ ] Unit tests for Gemini response & streaming handling
- [ ] Integration tests for end-to-end inference via OpenRouter

#### Groq Llama 3 Integration (`feat/groq-llama3`)

- [ ] Install `groq` Python client
- [ ] Initialise client using `GROQ_API_KEY` env variable
- [ ] Support `llama3-70b-8192` (and smaller) models
- [ ] Add provider adapter compatible with `LLMProvider` router
- [ ] Implement retry & timeout logic matching other providers
- [ ] Unit tests for Groq completion & error paths
- [ ] Integration tests comparing latency vs OpenRouter

### 3 · Schema & Docs Clean‑up (3C) - Detailed Tasks

#### Code Cleanup (`chore/drop-unused-models`)

- [ ] Remove `QueryLog` + stub `Vector` models
- [ ] Plan for real audit log implementation later
- [ ] Clean up dead code and unused imports

#### Incremental Ingestion (`feat/upsert-ingest`)

- [ ] Implement SHA-256 per page in ingestion script
- [ ] Add UPSERT logic for only new/changed chunks
- [ ] Add integration tests: update 1 page, DB rows count remains same

#### Documentation Sync (`chore/docs-sync`)

- [ ] Align environment variable names across docs
- [ ] Add doc-lint (markdown-link-check, doctoc) to pre-commit
- [ ] Add CI doc-lint job
- [ ] Update .env examples and README files

#### User & Developer Guides (`docs/user-dev-guides`)

- [ ] Author comprehensive API usage guide with request/response samples
- [ ] Document model selection & configuration options
- [ ] Provide Streamlit demo setup instructions for local & cloud
- [ ] Write deployment walkthrough (Docker, DigitalOcean/AWS)
- [ ] Add troubleshooting & FAQ section
- [ ] Include doc-lint coverage in CI

### 4 · Container Hardening (3D) - Detailed Tasks

#### Multi-stage Dockerfile (`infra/multistage-docker`)

- [ ] Implement builder → runtime stage separation
- [ ] Exclude dev dependencies from runtime
- [ ] Add non-root `app` user
- [ ] Add HEALTHCHECK endpoint
- [ ] CI job: ensure image < 400MB

#### Monitoring Integration (`infra/prometheus`)

- [ ] Add `prometheus_fastapi_instrumentator`
- [ ] Expose `/metrics` endpoint
- [ ] Add integration tests for scraping `http_requests_total`

#### Deployment Pipeline (`infra/deploy-public`)

- [ ] Containerise combined FastAPI & Streamlit services
- [ ] Configure GitHub Actions CI/CD on push to `main` branch
- [ ] Deploy automatically to **DigitalOcean** (App Platform / Droplet)  
      _Rationale: chosen for simpler setup & cost-efficiency for initial release_
- [ ] Run smoke tests post-deploy & report status
- [ ] Perform load testing and document scaling strategy

### 5 · Testing Expansion (4A) - Detailed Tasks

#### Unit Testing Expansion (`test/unit-index`)

- [ ] Implement PGVector in-memory mock (sqlite+vector)
- [ ] Add comprehensive unit test coverage
- [ ] Target: 80% line coverage, 90% function coverage on `app/`

#### Integration Testing (`test/integration-rag`)

- [ ] Set up Postgres/Redis via pytest-docker
- [ ] Implement end-to-end RAG pipeline tests
- [ ] Test ingestion → query → response flow

#### E2E Testing (`test/e2e-stream`)

- [ ] Test `/chat` endpoint with long questions
- [ ] Assert proper streaming behavior
- [ ] Validate full system integration

#### Security Testing (`test/fuzz-heuristic`)

- [ ] Implement random prompt injection string testing
- [ ] Ensure proper sanitization
- [ ] Test malformed PDF handling
- [ ] Add heuristic fuzz testing

#### Coverage Gates

- [ ] Implement `pytest --cov=app --cov-fail-under=80`
- [ ] Add coverage reporting to CI/CD
- [ ] Set up coverage gate in branch protection

### 6 · Memory & Doc Management (4B) - Detailed Tasks

#### Conversation Memory (`feat/redis-memory`)

- [ ] Store **last 20-30 Q&A pairs** per `session_id` in Redis for context window
- [ ] **Persist memory across browser sessions** with optional user reset button
- [ ] Implement **30-day retention policy** with automatic soft deletion of old conversations
- [ ] Prepend conversation context to prompts
- [ ] Add unit tests for context length logic
- [ ] Implement memory cleanup strategies

#### Document Management API (`feat/doc-upload-api`)

- [ ] Implement `/api/v1/docs` POST for new file uploads
- [ ] Add PUT endpoint for document versioning
- [ ] **Implement soft deletion only** - never remove documents from database
- [ ] **Client-side validation for file size limit (10MB max)**
- [ ] Add integration tests: upload → ingest → query returns new content
- [ ] Implement file validation and security

#### Metadata Schema (`feat/metadata-schema`)

- [ ] Add `documents` table (id, path, version, uploaded_at, author, **file_size, page_count, content_hash**)
- [ ] Support **free-text tagging** system with tag management
- [ ] **Keep all document versions** with soft delete flags
- [ ] Implement metadata CRUD operations
- [ ] Add integration tests for metadata management
- [ ] Support document tagging and categorization

#### Rate Limit Status API (`feat/rate-limit-status`)

- [ ] Implement `/api/v1/rate-limit/status` endpoint returning remaining requests & reset time
- [ ] Include current window usage and next reset timestamp
- [ ] Add caching to avoid Redis overhead on frequent status checks
- [ ] Unit tests for status calculation accuracy
- [ ] Integration tests for status endpoint

#### Streamlit Demo (`feat/demo-ui`)

- [ ] Create Streamlit-based demo interface (`streamlit-chat` / `hugchat`)
- [ ] Support file uploads **PDF only for v1** with **separate "Process" button workflow**
- [ ] **Client-side validation: reject PDFs >10MB** with clear error message
- [ ] Allow **users to input their own API keys** (stored in session state, cleared on exit)
- [ ] Implement **API key sanitization middleware** to redact keys from logs
- [ ] Implement chat interface using `/api/v1/chat` with conversation history display
- [ ] Add model selection dropdown with default options:
      - "Gemini (OpenRouter)"
      - "Llama 3 (Groq)"
      - "ChatGPT (OpenAI)"
      - "Local (GPT4All)"
- [ ] **Expose model parameters**: temperature, max_tokens sliders in sidebar
- [ ] Enable streaming responses for answers > 300 characters
- [ ] Display document source & tag metadata alongside responses
- [ ] **Show rate limit info in sidebar** via `/api/v1/rate-limit/status` calls
- [ ] **Sidebar rate limit warnings** when approaching limits
- [ ] Unit tests for each UI feature
- [ ] End-to-end tests for upload + chat interaction
- [ ] Rate-limit message UX improvements

### 7 · Optional Features (4C) - Deferred Tasks

#### User Feedback System

- [ ] Implement feedback collection API
- [ ] Integrate with Langfuse for quality tracking
- [ ] Add feedback-based model improvements

#### WordPress Plugin

- [ ] Develop WordPress integration
- [ ] Create plugin for easy embedding
- [ ] Document installation and configuration

#### Serverless PoC

- [ ] Explore serverless deployment options
- [ ] Implement cost optimization strategies
- [ ] Create deployment automation

#### Langfuse Deep Tracing (`feat/langfuse-trace`)

- [ ] Add `langfuse` SDK dependency and initialise client via settings (no hard-coded secrets)
- [ ] Decorate LLM & RAG calls with `@trace` to capture spans
- [ ] Implement **custom masking function** to redact sensitive data before transmission
- [ ] Ensure sampling configuration & redaction rules cover inputs/outputs & metadata
- [ ] Integration tests verifying traces appear in Langfuse dashboard
- [ ] Update docs with observability guidelines

#### DOCX & URL Ingestion (`feat/docx-url-ingest`)

- [ ] Extend ingestion pipeline to accept DOCX and remote URLs
- [ ] Update `/api/v1/docs` validation & chunking logic for new formats
- [ ] Update metadata schema to store original URL & file type
- [ ] Integration tests: ingest DOCX & URL then query returns content
- [ ] Update docs & demo once feature is stable

## Testing Methodologies

**Unit** – stateless pure functions; LLMs mocked

**Integration** – local Postgres / Redis via pytest‑docker; run ingest → query

**E2E** – docker‑compose full stack; HTTP smoke tests

**Heuristic / Fuzz** – adversarial prompts, malformed PDFs

**Coverage Gate** – pytest --cov=app --cov-fail-under=80

## Branch Protection & CI Matrix

| Job | Trigger | Steps |
|-----|---------|-------|
| lint | PR | ruff, pyupgrade, mypy |
| unit-test | PR | pytest -m unit |
| integration-test | PR label integration or main | spin DB / Redis, run tests |
| docker-build | tag on main | build, Trivy scan, push ghcr.io |
| doc-lint | PR | markdownlint‑cli2, link‑check |

## Git Branching & PR Workflow

```
main             # always deployable
└── develop      # integration
    ├── feat/<slug>/…
    ├── fix/<slug>/…
    ├── chore/<infra‑slug>/…
    └── test/<slug>/…
```

**Rule per PR**: One topic, ≤ 400 LoC, checklist must pass.

## Release Tags

**v0.x.y** # dev/demo
**v1.x.y** # first prod cut

Tag triggers Docker build → DigitalOcean Container Registry deploy.

## Development Commands (Phase 3)

```bash
make up           # start services
make down         # stop services
make logs         # view logs
make test         # all tests (≥14 pass)
make lint         # code quality
make format       # format code
make shell        # container shell
make db-shell     # psql
make ingest       # re‑ingest PDFs
# Baseline eval
docker exec chatbot-api-service-app-1 python scripts/run_baseline_evaluation.py
# Smoke test
curl -X POST "http://localhost:8000/api/v1/chat" \
     -H "Content-Type: application/json" \
     -H "X-API-Key: <key>" \
     -d '{"question":"test","session_id":"test"}'
```

## Quality Metrics (Achieved)

✅ **20+ tests** ✅ **Chat coverage** ✅ **Lint & type clean** ✅ **Secrets scan & API‑key auth** ✅ **Docs & guides** ✅ **Langfuse tracing** ✅ **Structured logging** (JSON format, correlation IDs, sensitive data masking)

## Immediate Priorities (Top 3)

1. **Langfuse Config Fix** – verify `NEXTAUTH_SECRET` and `SALT` are set, verify tracing, dataset creation.
2. **PGVector Migration** – branch fix/pgvector-type, vector → VECTOR(1536), ivfflat index.
3. **Docker Hardening** – branch infra/multistage-docker, image < 400 MB, add HEALTHCHECK + Trivy clean.

✅ **COMPLETED**: Redis Rate-Limit & Structured Logging

Complete in sequence; each merged PR closes its table entry.
