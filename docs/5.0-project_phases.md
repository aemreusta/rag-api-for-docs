# Project Phases

This updated plan leverages the new frameworks to accelerate development and focus on quality.

## Phase 1: Setup & Data Ingestion ‚úÖ **COMPLETED**

**Goal**: Establish the core infrastructure and create a high-quality, indexed knowledge base.

**Details**:

- ‚úÖ **Setup**: Configure the environment with FastAPI, LlamaIndex, Langfuse, Docker, Postgres, and Redis.
- ‚úÖ **Ingestion Script**: Refactor `scripts/ingest.py` to use LlamaIndex's PyMuPDFReader and SentenceSplitter (or a more advanced node parser).
- ‚úÖ **Indexing**: Configure the PGVectorStore in LlamaIndex and run the ingestion script to populate the database with embeddings.
- ‚úÖ **Observability**: Integrate the Langfuse SDK from day one to trace the ingestion process.
- ‚úÖ **Quality Assurance**: Comprehensive testing (7/7 tests passing), pre-commit hooks, and code quality validation.

**Outcome**: ‚úÖ **ACHIEVED** - A versioned, indexed knowledge base in PostgreSQL and a repeatable ingestion script.

**Current Status**:

- API server running on <http://localhost:8000>
- PostgreSQL with pgvector extension enabled
- Redis cache operational
- Langfuse observability platform running on <http://localhost:3000>
- PDF documents successfully indexed (sample_policy.pdf)
- All tests passing (35% code coverage)
- Pre-commit hooks and quality gates implemented

## Phase 2: Query Engine Development & Evaluation ‚úÖ **COMPLETED**

**Goal**: Build and validate the core RAG pipeline that can answer questions accurately.

### Todo List

- ‚úÖ **Build Core Query Engine**: Implement `core/query_engine.py` with:
  - ‚úÖ Initialize PGVectorStore with content_embeddings table
  - ‚úÖ Configure OpenRouter LLM integration
  - ‚úÖ Set up VectorStoreIndex
  - ‚úÖ Implement CondenseQuestionChatEngine for conversational context
  - ‚úÖ Expose get_chat_response function
- ‚úÖ **API Endpoint**: Wire the query engine to the FastAPI endpoint in `chat.py`.
- ‚úÖ **Create Dataset**: In Langfuse, create a "golden dataset" of 20-50 important questions with ideal answers.
- ‚úÖ **Evaluate**: Run your first evaluations using Langfuse to score the baseline engine on metrics like Faithfulness, Answer Relevancy, and Context Precision.

**Outcome**: ‚úÖ **ACHIEVED** - A functional API endpoint whose quality is tracked and measured.

**Completed Tasks**:

1. ‚úÖ ~~Implement chat endpoint in `app/api/v1/chat.py`~~
2. ‚úÖ ~~Wire query engine to FastAPI routes~~
3. ‚úÖ ~~Create evaluation dataset in Langfuse~~
4. ‚úÖ ~~Run initial quality assessments~~

**Current Status**:

- Chat API endpoint functional with authentication
- Query engine integrated with OpenRouter LLM
- Baseline evaluation script implemented
- Test coverage for chat functionality
- System ready for Phase 3 development

## Phase 3: Advanced Features & Deployment üöß **IN PROGRESS**

**Goal**: Implement conversational memory and deploy the secure, observable service.

### Todo List

- ‚úÖ **Conversational Memory**: Upgrade the QueryEngine to a ChatEngine using LlamaIndex's built-in memory components to handle follow-up questions.
  - ‚úÖ CondenseQuestionChatEngine already implemented in `query_engine.py`
  - ‚úÖ Session-based chat context handling
- ‚è≥ **Security & Rate Limiting**: Implement enhanced security and Redis-based rate limiting:
  - ‚úÖ API key authentication already implemented
  - ‚è≥ Implement Redis-based rate limiting per session/IP
  - ‚è≥ Add request validation middleware
  - ‚è≥ Implement CORS configuration for production
- ‚è≥ **Fix Langfuse Configuration**: Resolve observability issues:
  - ‚è≥ Fix CLICKHOUSE_URL configuration for Langfuse
  - ‚è≥ Test end-to-end Langfuse tracing
  - ‚è≥ Verify evaluation dataset creation in UI
- ‚è≥ **Production Readiness**: Prepare for deployment:
  - ‚è≥ Finalize Dockerfile optimization
  - ‚è≥ Update docker-compose.yml for production
  - ‚è≥ Add health checks and monitoring
  - ‚è≥ Environment-specific configuration
- ‚è≥ **API Documentation & Integration**: Prepare for frontend integration:
  - ‚è≥ Generate OpenAPI/Swagger documentation
  - ‚è≥ Create integration guide for frontend teams
  - ‚è≥ Document user feedback collection for Langfuse
  - ‚è≥ Add example API usage scripts

**Next Priority Tasks**:

1. Fix Langfuse observability configuration
2. Implement Redis-based rate limiting
3. Optimize production deployment setup
4. Generate comprehensive API documentation

**Outcome**: A smart, secure, and observable chatbot is live and accessible to the public.

## Phase 4: Continuous Improvement & Tuning ‚è≥ **PLANNED**

**Goal**: Use the observability platform to systematically improve the chatbot's performance, cost, and quality.

**Details**:

- **Monitor**: Actively monitor the Langfuse dashboards for cost spikes, latency degradation, or high rates of low-scoring user feedback.
- **Debug**: Use the detailed traces in Langfuse to analyze and fix any user-reported issues or bad responses.
- **A/B Test**: Use the Langfuse evaluation tools to A/B test different prompts, LLMs (via OpenRouter), or LlamaIndex retriever settings to find the optimal configuration.
- **Tune**: Based on data, make informed decisions to tune the system for better performance and user satisfaction.

**Outcome**: A mature, data-driven workflow for maintaining and enhancing a high-quality AI service.

## Development Commands for Current Phase

With Phase 1 and Phase 2 complete, Phase 3 in progress, these are the key commands for ongoing development:

```bash
# System Management
make up          # Start all services
make down        # Stop all services
make logs        # View service logs

# Development
make test        # Run tests (should show 14+ passing)
make lint        # Check code quality
make format      # Format code
make shell       # Access app container

# Database
make db-shell    # Access PostgreSQL
make ingest      # Re-run data ingestion

# Evaluation & Testing
docker exec chatbot-api-service-app-1 python scripts/run_baseline_evaluation.py  # Run baseline evaluation
curl -X POST "http://localhost:8000/api/v1/chat" -H "Content-Type: application/json" -H "X-API-Key: your-key" -d '{"question": "test", "session_id": "test"}'  # Test chat endpoint
```

## Quality Metrics Achieved

- ‚úÖ **Tests**: 14+ passing (expanded test coverage)
- ‚úÖ **Coverage**: Enhanced with chat endpoint testing
- ‚úÖ **Linting**: All checks pass (ruff, mypy, pyupgrade)
- ‚úÖ **Security**: Secrets scanning + API key authentication
- ‚úÖ **Documentation**: Comprehensive setup and evaluation guides
- ‚úÖ **Observability**: Langfuse integration (needs configuration fix)
- ‚úÖ **Evaluation Dataset**: v1-policy-questions dataset + baseline evaluation script
- ‚úÖ **API Functionality**: Chat endpoint with authentication and error handling
